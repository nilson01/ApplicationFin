{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9991a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf6a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7b71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "O1 = np.random.normal(0, 1, (sample_size, 3))\n",
    "A2 = np.random.randint(1, 4, sample_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46fd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "334364fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b48044d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 3, 3, 1, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab0e836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13827105, -1.13850132, -0.36779692],\n",
       "       [-0.48521878, -2.77699117, -0.34939154],\n",
       "       [-2.0250776 ,  0.60206749,  0.31531961],\n",
       "       [-0.30072282, -2.18131407, -0.86849781],\n",
       "       [-1.36589492, -0.97907894, -1.79591771],\n",
       "       [ 1.54883484,  0.89622265, -2.0370887 ],\n",
       "       [ 0.83768825, -0.18950545, -0.85639641],\n",
       "       [-0.66255101, -0.74248416,  1.18513487],\n",
       "       [-1.27920179, -0.5664314 , -0.97207485],\n",
       "       [ 2.09161351, -0.94204533,  0.11204933]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "343e8b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01911888, 0.23543726, 0.36248526, 0.75428845, 3.22532042,\n",
       "       2.39888936, 0.7017216 , 0.55128273, 1.63635722, 4.37484709])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract corresponding columns from O1 using A2 as indices\n",
    "O1[np.arange(sample_size), A2 - 1]**2\n",
    "\n",
    "# array([ 0.13827105, -0.48521878,  0.60206749, -0.86849781, -1.79591771,\n",
    "#         1.54883484,  0.83768825, -0.74248416, -1.27920179,  2.09161351])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65abcffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2354372644646884"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.48521878)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8fbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import norm\n",
    "from collections import Counter\n",
    "\n",
    "import pdb\n",
    "# # Set the seed for reproducibility\n",
    "# seed = 12345\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pip install rpy2\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import numpy2ri\n",
    "\n",
    "# Activate automatic conversion of numpy objects to R objects\n",
    "numpy2ri.activate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batches(N, batch_size, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, N, batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx+batch_size]\n",
    "        yield torch.tensor(batch_indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "def extract_and_prepare_data(A1_tensor_test, A2_tensor_test, A1, A2, d1_star, d2_star):\n",
    "\n",
    "    # Helper function to convert tensors to numpy arrays\n",
    "    def to_numpy(data):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return data.cpu().numpy() if data.is_cuda else data.numpy()\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            return data\n",
    "        else:\n",
    "            raise TypeError(\"The input must be a PyTorch tensor or a NumPy array\")\n",
    "\n",
    "    new_row = {\n",
    "        'Behavioral_A1': to_numpy(A1_tensor_test).tolist(),\n",
    "        'Behavioral_A2': to_numpy(A2_tensor_test).tolist(),\n",
    "        'Predicted_A1': to_numpy(A1).tolist(),\n",
    "        'Predicted_A2': to_numpy(A2).tolist(),\n",
    "        'Optimal_A1': to_numpy(d1_star).tolist(),\n",
    "        'Optimal_A2': to_numpy(d2_star).tolist()\n",
    "    }\n",
    "\n",
    "    return new_row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NNClass(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_networks, dropout_rate):\n",
    "        super(NNClass, self).__init__()\n",
    "        self.networks = nn.ModuleList()\n",
    "        for _ in range(num_networks):\n",
    "            network = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ELU(alpha=0.4),  # Using ELU instead of ReLU; best result 0.2, 0.13\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ELU(alpha=0.4),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_dim, output_dim),\n",
    "                nn.BatchNorm1d(output_dim),\n",
    "            )\n",
    "            self.networks.append(network)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for network in self.networks:\n",
    "            outputs.append(network(x))\n",
    "        return outputs\n",
    "\n",
    "    def he_initializer(self):\n",
    "        for network in self.networks:\n",
    "            for layer in network:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    nn.init.constant_(layer.bias, 0)  # Biases can be initialized to zero\n",
    "\n",
    "    def reset_weights(self):\n",
    "        for network in self.networks:\n",
    "            for layer in network:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.constant_(layer.weight, 0.1) # best 0.1\n",
    "                    nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## train V validation loss\n",
    "\n",
    "def plot_simulation_surLoss_losses_in_grid(selected_indices, losses_dict, cols=3):\n",
    "    # Calculate the number of rows needed based on the number of selected indices and desired number of columns\n",
    "    rows = len(selected_indices) // cols + (len(selected_indices) % cols > 0)\n",
    "\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))  # Adjust figure size as needed\n",
    "    fig.suptitle(f'Training and Validation Loss for Selected Simulations @ n_epoch = {n_epoch}')\n",
    "\n",
    "    # Flatten the axes array for easy indexing, in case of a single row or column\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        train_loss, val_loss = losses_dict[idx]\n",
    "\n",
    "        # Plot on the ith subplot\n",
    "        axes[i].plot(train_loss, label='Training')\n",
    "        axes[i].plot(val_loss, label='Validation')\n",
    "        axes[i].set_title(f'Simulation {idx}')\n",
    "        axes[i].set_xlabel('Epochs')\n",
    "        axes[i].set_ylabel('Loss')\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def transform_Y(Y1, Y2):\n",
    "\n",
    "    # Identify the minimum value among Y1 and Y2, only if they are negative\n",
    "    min_negative_Y = min(min(Y1.min(), 0), min(Y2.min(), 0))\n",
    "\n",
    "    # If the minimum value is negative, adjust Y1 and Y2 by adding the absolute\n",
    "    # value of min_negative_Y plus 1 to ensure all values are non-negative\n",
    "    if min_negative_Y < 0:\n",
    "        Y1_trans = Y1 - min_negative_Y + 1\n",
    "        Y2_trans = Y2 - min_negative_Y + 1\n",
    "    else:\n",
    "        # If there are no negative values, no adjustment is needed\n",
    "        Y1_trans = Y1\n",
    "        Y2_trans = Y2\n",
    "\n",
    "    return Y1_trans, Y2_trans\n",
    "\n",
    "\n",
    "def A_sim(matrix_pi, stage):\n",
    "    N, K = matrix_pi.shape  # sample size and treatment options\n",
    "    if N <= 1 or K <= 1:\n",
    "        raise ValueError(\"Sample size or treatment options are insufficient!\")\n",
    "    if np.min(matrix_pi) < 0:\n",
    "        raise ValueError(\"Treatment probabilities should not be negative!\")\n",
    "\n",
    "    # Normalize probabilities to add up to 1 and simulate treatment A for each row\n",
    "    pis = matrix_pi.sum(axis=1)\n",
    "    probs = np.divide(matrix_pi, pis[:, np.newaxis])\n",
    "    A = np.array([np.random.choice(np.arange(K), p=probs[i,]) for i in range(N)])\n",
    "    if stage == 1:\n",
    "        col_names = ['pi_10', 'pi_11', 'pi_12']\n",
    "    else:\n",
    "        col_names = ['pi_20', 'pi_21', 'pi_22']\n",
    "    probs_df = pd.DataFrame(probs, columns=col_names)\n",
    "    return {'A': A, 'probs': probs_df}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(sample_size, setting, replication_seed):\n",
    "    np.random.seed(replication_seed)\n",
    "\n",
    "    print(\"DGP: \", setting)\n",
    "\n",
    "    # Simulate baseline covariates\n",
    "    N = sample_size\n",
    "    x1, x2, x3, x4, x5 = np.random.normal(size=(5, N))\n",
    "    O1 = np.column_stack((x1, x2, x3, x4, x5))\n",
    "\n",
    "    Z1 = np.random.normal(0, 1, sample_size)\n",
    "    Z2 = np.random.normal(0, 1, sample_size)\n",
    "\n",
    "    if noiseless:\n",
    "        Z1 = 0\n",
    "        Z2 = 0\n",
    "\n",
    "    # Stage 1 data simulation\n",
    "    pi_10 = np.ones(N)\n",
    "    pi_11 = np.exp(0.5 - 0.5 * x3)\n",
    "    pi_12 = np.exp(0.5 * x4)\n",
    "    matrix_pi1 = np.column_stack((pi_10, pi_11, pi_12))\n",
    "    result1 = A_sim(matrix_pi1, stage=1)\n",
    "    A1, probs1 = result1['A'], result1['probs']\n",
    "    A1 +=1\n",
    "\n",
    "\n",
    "    # Optimal g1.opt corrected\n",
    "    g1_opt = ((x1 > -1).astype(float) * ((x2 > -0.5).astype(float) + (x2 > 0.5).astype(float))).astype(float) + 1\n",
    "\n",
    "\n",
    "    # Stage 1 outcome R1\n",
    "\n",
    "    Y1 = np.exp(1.5 - np.abs(1.5 * x1 + 2) * (A1 - g1_opt)**2) + Z1\n",
    "\n",
    "    # Stage 2 data simulation\n",
    "    pi_20 = np.ones(N)\n",
    "    pi_21 = np.exp(0.2 * Y1 - 1)\n",
    "    pi_22 = np.exp(0.5 * x4)\n",
    "    matrix_pi2 = np.column_stack((pi_20, pi_21, pi_22))\n",
    "    result2 = A_sim(matrix_pi2, stage=2)\n",
    "    A2, probs2 = result2['A'], result2['probs']\n",
    "    A2 +=1\n",
    "\n",
    "    Y1_opt = np.exp(1.5) + Z1\n",
    "\n",
    "    g2_opt = (x3 > -1).astype(float) * ((Y1_opt > 0.5).astype(float) + (Y1_opt > 3).astype(float)) + 1\n",
    "\n",
    "\n",
    "\n",
    "    # Stage 2 outcome R2\n",
    "    Y2 = np.exp(1.26 - np.abs(1.5 * x3 - 2) * (A2 - g2_opt)**2) + Z2\n",
    "\n",
    "\n",
    "    # Extract propensity for both stages\n",
    "    pi_10, pi_11, pi_12 = probs1['pi_10'], probs1['pi_11'], probs1['pi_12']\n",
    "    pi_20, pi_21, pi_22 = probs2['pi_20'], probs2['pi_21'], probs2['pi_22']\n",
    "\n",
    "    # Dummy O2\n",
    "    O2 = np.zeros(sample_size)\n",
    "\n",
    "    Y1_trans, Y2_trans = transform_Y(Y1, Y2)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'dgp': setting,\n",
    "        'O1': torch.tensor(O1, dtype=torch.float32, device=device),\n",
    "        'A1': torch.tensor(A1, dtype=torch.float32, device=device),\n",
    "        'Z1': torch.tensor(Z1, dtype=torch.float32, device=device),\n",
    "        'Y1': torch.tensor(Y1, dtype=torch.float32, device=device),\n",
    "        'O2': torch.tensor(O2, dtype=torch.float32, device=device),\n",
    "        'A2': torch.tensor(A2, dtype=torch.float32, device=device),\n",
    "        'Z2': torch.tensor(Z2, dtype=torch.float32, device=device),\n",
    "        'Y2': torch.tensor(Y2, dtype=torch.float32, device=device),\n",
    "        'pi_10': torch.tensor(pi_10, dtype=torch.float32, device=device),\n",
    "        'pi_11': torch.tensor(pi_11, dtype=torch.float32, device=device),\n",
    "        'pi_12': torch.tensor(pi_12, dtype=torch.float32, device=device),\n",
    "        'pi_20': torch.tensor(pi_20, dtype=torch.float32, device=device),\n",
    "        'pi_21': torch.tensor(pi_21, dtype=torch.float32, device=device),\n",
    "        'pi_22': torch.tensor(pi_22, dtype=torch.float32, device=device),\n",
    "        'Y1_trans': torch.tensor(Y1_trans, dtype=torch.float32, device=device),\n",
    "        'Y2_trans': torch.tensor(Y2_trans, dtype=torch.float32, device=device),\n",
    "        'g1_opt': torch.tensor(g1_opt, dtype=torch.float32, device=device),\n",
    "        'g2_opt': torch.tensor(g2_opt, dtype=torch.float32, device=device),\n",
    "    }\n",
    "\n",
    "\n",
    "# Preprocess\n",
    "def preprocess_data(generated_data, sample_size, setting, replication_seed, run='train', training_validation_prop = 0.8):\n",
    "\n",
    "    # Extract data from the generated_data dictionary\n",
    "    O1_tensor, O2_tensor = [generated_data[key] for key in ['O1', 'O2']]\n",
    "    A1_tensor, A2_tensor = [generated_data[key] for key in ['A1', 'A2']]\n",
    "    g1_opt, g2_opt = [generated_data[key] for key in ['g1_opt', 'g2_opt']]\n",
    "\n",
    "\n",
    "    if run == 'test':\n",
    "        Y1_tensor, Y2_tensor = [generated_data[key] for key in ['Y1', 'Y2']]\n",
    "    else:\n",
    "        Y1_tensor, Y2_tensor = [generated_data[key] for key in ['Y1_trans', 'Y2_trans']]\n",
    "\n",
    "    Y1_tensor_opt = torch.exp(torch.tensor(1.5)) + generated_data['Z1']\n",
    "    \n",
    "    if tree_type:\n",
    "        g1_opt = ((O1_tensor[:, 0] > -1).float() * ((O1_tensor[:, 1] > -0.5).float() + (O1_tensor[:, 1] > 0.5).float())) + 1\n",
    "\n",
    "        g2_opt = ((O1_tensor[:, 2] > -1).float() * ((Y1_tensor_opt > 0.5).float() + (Y1_tensor_opt > 3).float())) + 1\n",
    "    else:\n",
    "\n",
    "        g1_opt = ((O1_tensor[:, 0] > -0.5).float() * (1 + (O1_tensor[:, 0] - O1_tensor[:, 1] > 0).float())) + 1\n",
    "\n",
    "        g2_opt = ((O1_tensor[:, 2] > 0).float() + ((O1_tensor[:, 2] + Y1_tensor_opt > 2.5).float())) + 1\n",
    "\n",
    "    Y2_tensor = torch.exp(torch.tensor(1.26) - torch.abs(torch.tensor(1.5) * O1_tensor[:, 2] - 2) * (A2_tensor - g2_opt)**2) + generated_data[\"Z2\"]\n",
    "\n",
    "\n",
    "    # Probabilities\n",
    "    pi_tensors = [generated_data[key] for key in ['pi_10', 'pi_11', 'pi_12', 'pi_20', 'pi_21', 'pi_22']]\n",
    "    pi_tensor_stack = torch.stack(pi_tensors)\n",
    "\n",
    "    # Adjusting A1 and A2 indices\n",
    "    A1_indices = (A1_tensor - 1).long().unsqueeze(0)  # A1 actions, Subtract 1 to match index values (0, 1, 2)\n",
    "    A2_indices = (A2_tensor - 1 + 3).long().unsqueeze(0)   # A2 actions, Add +3 to match index values (3, 4, 5) for A2, with added dimension\n",
    "\n",
    "    # Gathering probabilities based on actions\n",
    "    # Since we're gathering along the first dimension, we need to use the dim=0 parameter in torch.gather\n",
    "    P_A1_given_H1_tensor = torch.gather(pi_tensor_stack, dim=0, index=A1_indices).squeeze(0)  # Remove the added dimension after gathering\n",
    "    P_A2_given_H2_tensor = torch.gather(pi_tensor_stack, dim=0, index=A2_indices).squeeze(0)  # Remove the added dimension after gathering\n",
    "\n",
    "    # Calculate Ci tensor\n",
    "    Ci_tensor = (Y1_tensor + Y2_tensor) / (P_A1_given_H1_tensor * P_A2_given_H2_tensor)\n",
    "\n",
    "    # Input preparation\n",
    "    input_stage1 = O1_tensor\n",
    "\n",
    "    if setting == 'tao':\n",
    "        input_stage2 = torch.cat([O1_tensor, A1_tensor.unsqueeze(1), Y1_tensor.unsqueeze(1)], dim=1)\n",
    "    else:\n",
    "        input_stage2 = torch.cat([O1_tensor, A1_tensor.unsqueeze(1), Y1_tensor.unsqueeze(1), O2_tensor.unsqueeze(1)], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    if run == 'test':\n",
    "        return input_stage1, input_stage2, Ci_tensor\n",
    "\n",
    "    # print(\"input_stage2: \", input_stage2[:5, :])\n",
    "\n",
    "    # Splitting data into training and validation sets\n",
    "    train_size = int(training_validation_prop * sample_size)\n",
    "    train_tensors = [tensor[:train_size] for tensor in [input_stage1, input_stage2, Ci_tensor, Y1_tensor, Y2_tensor, A1_tensor, A2_tensor]]\n",
    "    val_tensors = [tensor[train_size:] for tensor in [input_stage1, input_stage2, Ci_tensor, Y1_tensor, Y2_tensor, A1_tensor, A2_tensor]]\n",
    "\n",
    "    return tuple(train_tensors), tuple(val_tensors), tuple([input_stage1, input_stage2, Ci_tensor, Y1_tensor, Y2_tensor, A1_tensor, A2_tensor, pi_tensor_stack, g1_opt, g2_opt])\n",
    "\n",
    "\n",
    "def adaptive_contrast_tao(all_data, contrast):\n",
    "\n",
    "    train_input_stage1, train_input_stage2, train_Ci, train_Y1, train_Y2, train_A1, train_A2, pi_tensor_stack, g1_opt, g2_opt = all_data\n",
    "\n",
    "    A1 = train_A1.numpy()\n",
    "    probs1 = pi_tensor_stack.T[:, :3].numpy()\n",
    "\n",
    "    A2 = train_A2.numpy()\n",
    "    probs2 = pi_tensor_stack.T[:, 3:].numpy()\n",
    "\n",
    "    R1 = train_Y1.numpy()\n",
    "    R2 = train_Y2.numpy()\n",
    "\n",
    "    g1_opt = g1_opt.numpy()\n",
    "    g2_opt = g2_opt.numpy()\n",
    "\n",
    "\n",
    "\n",
    "    if setting == 'tao':\n",
    "\n",
    "        # Convert tensor to numpy if not already numpy array\n",
    "        train_input_np = train_input_stage1.numpy()\n",
    "\n",
    "        x1 = train_input_np[:, 0]\n",
    "        x2 = train_input_np[:, 1]\n",
    "        x3 = train_input_np[:, 2]\n",
    "        x4 = train_input_np[:, 3]\n",
    "        x5 = train_input_np[:, 4]\n",
    "\n",
    "\n",
    "        # Load the R script containing the function\n",
    "        ro.r('source(\"ACWL_tao.R\")')\n",
    "\n",
    "        # Call the R function\n",
    "        results = ro.globalenv['train_ACWL'](x1, x2, x3, x4, x5, A1, probs1, A2, probs2, R1, R2, g1_opt, g2_opt, contrast, method= f_model)\n",
    "\n",
    "\n",
    "    elif setting == 'linear':\n",
    "\n",
    "        # Convert tensor to numpy if not already numpy array\n",
    "        train_input_np = train_input_stage2.numpy()\n",
    "\n",
    "        x1 = train_input_np[:, 0]\n",
    "        x2 = train_input_np[:, 1]\n",
    "        O2 = train_input_np[:, 4]\n",
    "\n",
    "\n",
    "        # Load the R script containing the function\n",
    "        ro.r('source(\"ACWL_linear.R\")')\n",
    "\n",
    "        # Call the R function\n",
    "        results = ro.globalenv['train_ACWL'](x1, x2, O2, A1, probs1, A2, probs2, R1, R2, g1_opt, g2_opt, contrast, method= f_model)\n",
    "\n",
    "    # Extract results\n",
    "    select2 = results.rx2('select2')[0]\n",
    "    select1 = results.rx2('select1')[0]\n",
    "    selects = results.rx2('selects')[0]\n",
    "\n",
    "    return select2, select1, selects\n",
    "\n",
    "\n",
    "\n",
    "def prepare_stage2_test_input(generated_test_data, A1):\n",
    "    # Extract tensors from the generated_test_data dictionary\n",
    "    O1_tensor_test, O2_tensor_test = [generated_test_data[key] for key in ['O1', 'O2']]\n",
    "    Z1_tensor_test = generated_test_data['Z1']\n",
    "\n",
    "    # Optimal policy conditions for Stage 1\n",
    "    if tree_type:\n",
    "        g1_opt_conditions = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5).float() + (O1_tensor_test[:, 1] > 0.5).float())) + 1\n",
    "    else:\n",
    "        g1_opt_conditions = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0).float())) + 1\n",
    "\n",
    "\n",
    "    # Assuming g1_opt_conditions gives the optimal action, we use it to compute Y1_pred\n",
    "    if noiseless:\n",
    "        Z1_tensor_test = 0\n",
    "\n",
    "    Y1_pred = torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (A1 - g1_opt_conditions)**2) + Z1_tensor_test\n",
    "\n",
    "        \n",
    "    test_input_stage2 = torch.cat([O1_tensor_test, A1.unsqueeze(1), Y1_pred.unsqueeze(1)], dim=1)\n",
    "    #print(\"Y1_pred [min, max, mean]: \", [torch.min(Y1_pred), torch.max(Y1_pred), torch.mean(Y1_pred)], torch.min(torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (A1 - g1_opt_conditions)**2)),  torch.min(Z1_tensor_test))\n",
    "    # Calculate the required quantities\n",
    "    Y1_stats = [torch.min(Y1_pred), torch.max(Y1_pred), torch.mean(Y1_pred)]\n",
    "    exp_calculation = torch.min(torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (A1 - g1_opt_conditions)**2))\n",
    "    #Z1_min = torch.min(Z1_tensor_test)\n",
    "\n",
    "    # Construct the message string\n",
    "    stats_message = f\"Y1_pred [min, max, mean]: {Y1_stats}\"\n",
    "\n",
    "    # Use tqdm.write() to print the stats\n",
    "    tqdm.write(stats_message)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return test_input_stage2, Y1_pred\n",
    "\n",
    "\n",
    "\n",
    "def prepare_Y2_pred(generated_test_data, A1, A2):\n",
    "    O1_tensor_test = generated_test_data['O1']\n",
    "    Z1_tensor_test = generated_test_data['Z1']\n",
    "    Z2_tensor_test = generated_test_data['Z2']  # Assuming Z2 is a key in your dictionary\n",
    "\n",
    "\n",
    "    if tree_type:\n",
    "        Y1_pred = torch.exp(torch.tensor(1.5)) + Z1_tensor_test\n",
    "\n",
    "        g2_opt_conditions = ((O1_tensor_test[:, 2] > -1).float() * ((Y1_pred > 0.5).float() + (Y1_pred > 3).float())) + 1\n",
    "\n",
    "    else:\n",
    "        Y1_pred = torch.exp(torch.tensor(1.5)) #torch.exp(1.5) #+ Z1_tensor_test\n",
    "\n",
    "        g2_opt_conditions = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + Y1_pred > 2.5).float())) + 1\n",
    "\n",
    "    # Assuming g2_opt_conditions gives the optimal action, we use it to compute Y2_pred\n",
    "    if noiseless:\n",
    "        Z2_tensor_test = 0\n",
    "\n",
    "    Y2_pred = torch.exp(1.26 - torch.abs(1.5 * O1_tensor_test[:, 2] - 2) * (A2 - g2_opt_conditions)**2) + Z2_tensor_test\n",
    "\n",
    "    # print(\"Y2_pred [min, max, mean]: \", [torch.min(Y2_pred), torch.max(Y2_pred), torch.mean(Y2_pred)] )\n",
    "    stats_message = f\"Y2_pred [min, max, mean]: [{torch.min(Y2_pred)}, {torch.max(Y2_pred)}, {torch.mean(Y2_pred)}]\"\n",
    "    tqdm.write(stats_message)\n",
    "\n",
    "    return Y2_pred\n",
    "\n",
    "\n",
    "\n",
    "# def compute_optimal_policy(generated_test_data, A1, A2):\n",
    "def compute_optimal_policy(generated_test_data):\n",
    "    O1_tensor_test = generated_test_data['O1']\n",
    "    Z1_tensor_test = generated_test_data['Z1']\n",
    "\n",
    "    Y1_pred = torch.exp(torch.tensor(1.5)) + Z1_tensor_test\n",
    "    if tree_type:\n",
    "        d1_star = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5).float() + (O1_tensor_test[:, 1] > 0.5).float())) + 1\n",
    "        d2_star = ((O1_tensor_test[:, 2] > -1).float() * ((Y1_pred > 0.5).float() + (Y1_pred > 3).float())) + 1\n",
    "    else:\n",
    "        d1_star = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0).float())) + 1\n",
    "        d2_star = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + Y1_pred > 2.5).float())) + 1\n",
    "\n",
    "    return d1_star, d2_star\n",
    "\n",
    "\n",
    "\n",
    "def calculate_optimal_policy_values(d1_star, d2_star, generated_test_data):\n",
    "\n",
    "    # Extract necessary tensors from the generated_test_data dictionary\n",
    "    O1_tensor_test, O2_tensor_test, Z1_tensor_test, Z2_tensor_test = [generated_test_data[key] for key in ['O1', 'O2', 'Z1', 'Z2']]\n",
    "\n",
    "\n",
    "    if tree_type:\n",
    "        g1_opt_conditions = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5).float() + (O1_tensor_test[:, 1] > 0.5).float())) + 1\n",
    "    else:\n",
    "        g1_opt_conditions = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0).float())) + 1\n",
    "\n",
    "\n",
    "    # Calculate Y1_test_opt and Y2_test_opt using the determined g1_opt and g2_opt\n",
    "    Y1_test_opt = torch.exp(torch.tensor(1.5) - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (d1_star - g1_opt_conditions)**2) + Z1_tensor_test\n",
    "\n",
    "    if tree_type:\n",
    "        g2_opt_conditions = ((O1_tensor_test[:, 2] > -1).float() * ((Y1_test_opt > 0.5).float() + (Y1_test_opt > 3).float())) + 1\n",
    "    else:\n",
    "        g2_opt_conditions = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + Y1_test_opt > 2.5).float())) + 1\n",
    "\n",
    "    Y2_test_opt = torch.exp(torch.tensor(1.26) - torch.abs(1.5 * O1_tensor_test[:, 2] - 2) * (d2_star - g2_opt_conditions)**2) + Z2_tensor_test\n",
    "\n",
    "    # the following is simplified work; above is just for clarity\n",
    "    # Y1_test_opt = torch.exp(torch.tensor(1.5)) + Z1_tensor_test\n",
    "    # Y2_test_opt = torch.exp(torch.tensor(1.26)) + Z2_tensor_test\n",
    "    test1 = torch.abs(1.5 * O1_tensor_test[:, 2] - 2) * (d2_star - g2_opt_conditions)**2\n",
    "\n",
    "    # Aggregate the values into a tuple\n",
    "    values_opt = (d1_star, O1_tensor_test, Z1_tensor_test, d2_star, Z2_tensor_test)\n",
    "\n",
    "    return Y1_test_opt, Y2_test_opt, values_opt\n",
    "\n",
    "\n",
    "\n",
    "def calculate_policy_values(d1_star, d2_star, generated_test_data, Y1_pred, Y2_pred, V_replications):\n",
    "    # Optimal policy value calculation\n",
    "    Y1_test_opt, Y2_test_opt, values_opt = calculate_optimal_policy_values(d1_star, d2_star, generated_test_data)\n",
    "    V_d1_d2_opt = torch.mean(Y1_test_opt + Y2_test_opt).cpu().item()  # Calculate the mean value and convert to Python scalar\n",
    "    V_replications[\"V_replications_M1_optimal\"].append(V_d1_d2_opt)  # Append to the list for optimal policy values\n",
    "\n",
    "    # Behavioral policy value calculation\n",
    "    V_d1_d2 = torch.mean(generated_test_data['Y1'] + generated_test_data['Y2']).cpu().item()  # Calculate the mean value and convert to Python scalar\n",
    "    V_replications[\"V_replications_M1_behavioral\"].append(V_d1_d2)  # Append to the list for behavioral policy values\n",
    "\n",
    "    # Current approach value calculation\n",
    "    V_replications[\"V_replications_M1_pred\"].append(torch.mean(Y1_pred + Y2_pred).item())  # Append the mean value as a Python scalar to the list for current approach values\n",
    "\n",
    "    return V_replications, values_opt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_DTR(sample_size, V_replications, num_replications, nn_stage1, nn_stage2, df, params):\n",
    "\n",
    "\n",
    "    # Calculate V using the best model parameters on the test data # Calculate V using the best model parameters on the test data\n",
    "    generated_test_data = generate_data(sample_size, params['setting'], replication_seed = num_replications)\n",
    "    # generated_test_data = generate_data_test(sample_size, params['setting'], replication_seed = num_replications)\n",
    "\n",
    "    # Preprocess data\n",
    "    test_input_stage1, test_input_stage2, Ci_tensor = preprocess_data(generated_test_data, sample_size, params['setting'], replication_seed=num_replications, run='test')\n",
    "    # test_input_stage1, test_input_stage2, Ci_tensor = preprocess_data_test(generated_test_data, params['setting'], replication_seed=num_replications, run='test')\n",
    "\n",
    "    A1_tensor_test, A2_tensor_test = [generated_test_data[key] for key in ['A1', 'A2']]\n",
    "\n",
    "\n",
    "    # optimal policy\n",
    "    d1_star, d2_star =  compute_optimal_policy(generated_test_data)\n",
    "\n",
    "\n",
    "    # Calculate test outputs for all networks in stage 1\n",
    "    # Perform forward pass\n",
    "    test_input_np = test_input_stage1.numpy()\n",
    "    x1 = test_input_np[:, 0]\n",
    "    x2 = test_input_np[:, 1]\n",
    "    x3 = test_input_np[:, 2]\n",
    "    x4 = test_input_np[:, 3]\n",
    "    x5 = test_input_np[:, 4]\n",
    "\n",
    "\n",
    "    # Load the R script containing the function\n",
    "    ro.r('source(\"ACWL_tao.R\")')\n",
    "\n",
    "    # Call the R function\n",
    "    results = ro.globalenv['test_ACWL'](x1, x2, x3, x4, x5, d1_star.numpy(), d2_star.numpy(), noiseless, method= f_model)\n",
    "\n",
    "    # Extract results\n",
    "\n",
    "    select2_test = results.rx2('select2')[0]\n",
    "    select1_test = results.rx2('select1')[0]\n",
    "    selects_test = results.rx2('selects')[0]\n",
    "\n",
    "    # TODO: FIX THESE TO GET EXACTLY SAME ACCURACY AS WE GET IN PYTHON\n",
    "    print(f\"TEST: Select1: {select1_test}, Select2: {select2_test}, Selects: {selects_test}\")\n",
    "\n",
    "\n",
    "    # Extracting each component of the results and convert them to tensors\n",
    "    Y1_pred_R = torch.tensor(np.array(results.rx2('R1.a1')), dtype=torch.float32)\n",
    "    Y2_pred_R = torch.tensor(np.array(results.rx2('R2.a1')), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    # TODO: FIX THESE TO GET EXACTLY SAME ACCURACY AS WE GET IN PYTHON\n",
    "\n",
    "    Y1_stats_R = [torch.min(Y1_pred_R), torch.max(Y1_pred_R), torch.mean(Y1_pred_R)]\n",
    "    message = f\"Y1_pred_R [min, max, mean]: {Y1_stats_R}\"\n",
    "    tqdm.write(message)\n",
    "    message = f\"Y2_pred_R [min, max, mean]: [{torch.min(Y2_pred_R)}, {torch.max(Y2_pred_R)}, {torch.mean(Y2_pred_R)}]\"\n",
    "    tqdm.write(message)\n",
    "\n",
    "\n",
    "    # torch.mean(Y1_pred + Y2_pred): 4.660262107849121\n",
    "    message = f'torch.mean(Y1_pred_R + Y2_pred_R): {torch.mean(Y1_pred_R + Y2_pred_R)} \\n'\n",
    "    tqdm.write(message)\n",
    "\n",
    "\n",
    "\n",
    "    A1 = torch.tensor(np.array(results.rx2('g1.a1')), dtype=torch.float32)\n",
    "    A2 = torch.tensor(np.array(results.rx2('g2.a1')), dtype=torch.float32)\n",
    "\n",
    "    test_input_stage2, Y1_pred = prepare_stage2_test_input(generated_test_data, A1)\n",
    "    Y2_pred =  prepare_Y2_pred(generated_test_data, A1, A2)\n",
    "\n",
    "\n",
    "\n",
    "    # optimal policy\n",
    "    # d1_star, d2_star =  compute_optimal_policy(generated_test_data, A1, A2)\n",
    "\n",
    "    # Append to DataFrame\n",
    "    new_row = {\n",
    "        'Behavioral_A1': A1_tensor_test.cpu().numpy().tolist(),\n",
    "        'Behavioral_A2': A2_tensor_test.cpu().numpy().tolist(),\n",
    "        'Predicted_A1': A1.cpu().numpy().tolist(),\n",
    "        'Predicted_A2':  A2.cpu().numpy().tolist(),\n",
    "        'Optimal_A1': d1_star.cpu().numpy().tolist(),\n",
    "        'Optimal_A2': d2_star.cpu().numpy().tolist()\n",
    "        }\n",
    "\n",
    "    # new_row = extract_and_prepare_data(A1_tensor_test, A2_tensor_test, A1, A2, d1_star, d2_star)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    message = f'torch.mean(Y1_pred + Y2_pred): {torch.mean(Y1_pred + Y2_pred)} \\n\\n'\n",
    "    #message = f'torch.mean(Y1_pred + Y2_pred): {torch.mean(Y1_pred + Y2_pred).item() if isinstance(Y1_pred, torch.Tensor) else np.mean(Y1_pred + Y2_pred)}'\n",
    "    tqdm.write(message)\n",
    "\n",
    "    V_replications, values_opt = calculate_policy_values(d1_star, d2_star, generated_test_data, Y1_pred, Y2_pred, V_replications)\n",
    "\n",
    "\n",
    "    return V_replications, df, values_opt\n",
    "\n",
    "\n",
    "def simulations(surrogate_num, sample_size, num_replications, V_replications, params):\n",
    "    columns = ['Behavioral_A1', 'Behavioral_A2', 'Predicted_A1', 'Predicted_A2', 'Optimal_A1', 'Optimal_A2']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    losses_dict = {}\n",
    "    epoch_num_model_lst = []\n",
    "\n",
    "    for replication in tqdm(range(num_replications), desc=\"Replications_M1\"):\n",
    "\n",
    "        # Generate data\n",
    "        generated_data = generate_data(sample_size, params['setting'],replication_seed=replication)\n",
    "        # Preprocess data (moved to GPU)\n",
    "        tuple_train, tuple_Val, all_data = preprocess_data(generated_data, sample_size, params['setting'], replication_seed=replication)\n",
    "\n",
    "        #  Estimate treatment regime: params['f_model']\n",
    "        (select2, select1, selects) = adaptive_contrast_tao(all_data, params[\"contrast\"])\n",
    "        # eval_DTR\n",
    "        V_replications, df, values_opt = eval_DTR(sample_size, V_replications, replication, None, None, df, params)\n",
    "\n",
    "\n",
    "    return V_replications, df, values_opt, losses_dict, epoch_num_model_lst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "sample_size = 1000  # 500, 1000 are the cases to check\n",
    "batch_prop = 0.2 #0.07, 0.2\n",
    "if sample_size < 500:\n",
    "    batch_prop = 0.5\n",
    "\n",
    "training_validation_prop = 0.5 #0.95 #0.01\n",
    "\n",
    "# Prompt user for the number of replications\n",
    "num_replications = 4\n",
    "\n",
    "# Prompt user for the setting\n",
    "setting = 'tao' # 'linear', 'tao', 'scheme_i'\n",
    "\n",
    "noiseless = True # True False\n",
    "\n",
    "\n",
    "\n",
    "if setting == 'tao':\n",
    "    tree_type =  True # True False\n",
    "\n",
    "# Prompt user for the model type\n",
    "f_model = 'tao' # (linear, 'tao', 'DQlearning', 'surr_opt'): \" tao => adaptive_contrast_tao) # Note for linear linear run separate R code\n",
    "\n",
    "contrast = 1\n",
    "\n",
    "surrogate_num = 1 #1- old multiplicative one  2- new one\n",
    "\n",
    "option_sur = 1 # if surrogate_num = 1 then from 1-5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "\n",
    "\n",
    "# Constants from scheme_i\n",
    "C1 = C2 = 3\n",
    "beta = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #BEST so far for tao's 15000\n",
    "\n",
    "network_parameters_surogate = {\n",
    "  'setting': setting,\n",
    "  'n_epoch': 60, #250\n",
    "  'num_networks': 2,\n",
    "  'input_dim_stage1': 2,\n",
    "  'output_dim_stage1': 1,\n",
    "  'input_dim_stage2': 5,\n",
    "  'output_dim_stage2': 1,\n",
    "  'optimizer_betas': (0.9, 0.999),\n",
    "  'optimizer_eps': 1e-08,\n",
    "  'scheduler_step_size': 30,\n",
    "  'scheduler_gamma': 0.8,\n",
    "  'hidden_dim_stage1': 10, #20\n",
    "  'hidden_dim_stage2': 10, #20\n",
    "  'dropout_rate': 0.0, #0.3, 0.43\n",
    "  'optimizer_lr': 0.07, # 0.07, 0.007\n",
    "  'optimizer_weight_decay': 0.001,\n",
    "  'batch_size': math.ceil(batch_prop*sample_size), #int(0.038*sample_size),\n",
    "  'f_model': 'surr_opt',\n",
    "  'option_sur': option_sur, # if surrogate_num = 1 then 5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "  'contrast': contrast\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_stage2 = [O1, A1, Y1, O2]\n",
    "\n",
    "if setting =='tao':\n",
    "    network_parameters_surogate['input_dim_stage1'] = 5\n",
    "    network_parameters_surogate['input_dim_stage2'] = 7\n",
    "\n",
    "\n",
    "network_parameters_surogate['option_sur'] = 2 # symmetric\n",
    "n_epoch = network_parameters_surogate['n_epoch']\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Lists to store replication results\n",
    "V_replications_M1_behavioral = []\n",
    "V_replications_M1_pred = []\n",
    "V_replications_M1_optimal = []\n",
    "\n",
    "V_replications = {\"V_replications_M1_behavioral\": V_replications_M1_behavioral, \"V_replications_M1_pred\": V_replications_M1_pred, \"V_replications_M1_optimal\": V_replications_M1_optimal}\n",
    "\n",
    "\n",
    "print('Setting: ' , setting)\n",
    "print(\"f_model: \", f_model)\n",
    "\n",
    "\n",
    "network_parameters_surogate['f_model'] = 'tao'\n",
    "V_replications, df, values_opt, losses_dict, epoch_num_model_lst = simulations(surrogate_num,\n",
    "                                                                           sample_size,\n",
    "                                                                           num_replications,\n",
    "                                                                           V_replications,\n",
    "                                                                           network_parameters_surogate)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3c5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72df28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e1a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7000fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fc10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31046689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d708135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de96e102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nilson/Desktop/DTR project/0.DirectSearchApplication'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507ae8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"data/None\"\n",
    "folder = \"data/20240802130249\"\n",
    "\n",
    "# Define paths to the files\n",
    "# df_path = os.path.join(folder, 'simulation_data.pkl')\n",
    "df_path = os.path.join(folder, 'simulation_results.pkl')\n",
    "\n",
    "# Load DataFrame\n",
    "# global_df = pd.read_csv(df_path)\n",
    "with open(df_path, 'rb') as f:\n",
    "    global_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b14969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'{\"activation_function\": \"elu\", \"batch_size\": 8000, \"learning_rate\": 0.07, \"num_layers\": 2}': {'DQL': {\"Method's Value fn.\": 7.881669173638026,\n",
       "   'Behavioral Value fn.': 3.68256147702535},\n",
       "  'DS': {\"Method's Value fn.\": 7.829710245132446,\n",
       "   'Behavioral Value fn.': 3.68256147702535}},\n",
       " '{\"activation_function\": \"elu\", \"batch_size\": 7000, \"learning_rate\": 0.07, \"num_layers\": 2}': {'DQL': {\"Method's Value fn.\": 7.874249408642451,\n",
       "   'Behavioral Value fn.': 3.6805241107940674},\n",
       "  'DS': {\"Method's Value fn.\": 7.81242357691129,\n",
       "   'Behavioral Value fn.': 3.6805241107940674}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7495c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5250c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2877652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e620996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c5588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf4a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_treatment_values(df, columns_to_process):\n",
    "    unique_values = {}\n",
    "\n",
    "    for key, cols in columns_to_process.items():\n",
    "        unique_values[key] = {}\n",
    "        \n",
    "        for col in cols:\n",
    "            all_values = [item for sublist in df[col] for item in sublist]\n",
    "            unique_values[key][col] = set(all_values)\n",
    "\n",
    "    return unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83d1fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Predicted': {'Predicted_A1': {1, 2, 3}, 'Predicted_A2': {1, 2, 3}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folder = \"data/None\"\n",
    "folder = \"data/20240710213120\"\n",
    "\n",
    "# Define paths to the files\n",
    "# df_path = os.path.join(folder, 'simulation_data.pkl')\n",
    "df_path = os.path.join(folder, 'simulation_data_DS.pkl')\n",
    "\n",
    "# Load DataFrame\n",
    "# global_df = pd.read_csv(df_path)\n",
    "with open(df_path, 'rb') as f:\n",
    "    global_df = pickle.load(f)\n",
    "\n",
    "# Extract and process unique values\n",
    "columns_to_process = {\n",
    "    'Predicted': ['Predicted_A1', 'Predicted_A2'],\n",
    "}\n",
    "unique_values = extract_unique_treatment_values(global_df, columns_to_process)\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05ffab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values:\n",
      "Predicted: {'Predicted_A1': {1, 2, 3}, 'Predicted_A2': {1, 2, 3}}\n",
      "\n",
      "\n",
      "Unique_values:  {'Predicted': {'Predicted_A1': {1, 2, 3}, 'Predicted_A2': {1, 2, 3}}} \n"
     ]
    }
   ],
   "source": [
    "# unique_values_ = {\"key1\": \"value1\", \"key2\": \"value2\"}  # Example dictionary\n",
    "log_message = \"\\nUnique values:\\n\" + \"\\n\".join(f\"{k}: {v}\" for k, v in unique_values.items()) + \"\\n\"\n",
    "print(log_message)\n",
    "print(f\"\\nUnique_values:  {unique_values} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8bc308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afab5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ad7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the files for both DQL and DS\n",
    "\n",
    "results_path = os.path.join(folder, 'simulation_results.pkl')\n",
    "\n",
    "\n",
    "with open(results_path, 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de519a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['{\"activation_function\": \"relu\", \"batch_size\": 3072, \"learning_rate\": 0.007, \"num_layers\": 4}'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e71081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'{\"activation_function\": \"relu\", \"batch_size\": 3072, \"learning_rate\": 0.007, \"num_layers\": 4}': {'DQL': {\"Method's Value fn.\": 7.834487060705821,\n",
       "   'Behavioral Value fn.': 3.681306759516398},\n",
       "  'DS': {\"Method's Value fn.\": 6.1342741052309675,\n",
       "   'Behavioral Value fn.': 3.681306759516398}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd825ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Method's Value fn.\": 7.834487060705821,\n",
       " 'Behavioral Value fn.': 3.681306759516398}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['{\"activation_function\": \"relu\", \"batch_size\": 3072, \"learning_rate\": 0.007, \"num_layers\": 4}'][\"DQL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9e164e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Method's Value fn.\": 6.1342741052309675,\n",
       " 'Behavioral Value fn.': 3.681306759516398}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['{\"activation_function\": \"relu\", \"batch_size\": 3072, \"learning_rate\": 0.007, \"num_layers\": 4}'][\"DS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e0ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DQL': {\"Method's Value fn.\": 7.834487060705821, 'Behavioral Value fn.': 3.681306759516398}, 'DS': {\"Method's Value fn.\": 6.1342741052309675, 'Behavioral Value fn.': 3.681306759516398}}\n"
     ]
    }
   ],
   "source": [
    "for config_key, performance in results.items():\n",
    "    print(performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08f0af9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'{\"activation_function\": \"relu\", \"batch_size\": 3072, \"learning_rate\": 0.007, \"num_layers\": 4}': {'DQL': {\"Method's Value fn.\": 7.834487060705821,\n",
       "   'Behavioral Value fn.': 3.681306759516398},\n",
       "  'DS': {\"Method's Value fn.\": 6.1342741052309675,\n",
       "   'Behavioral Value fn.': 3.681306759516398}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77dc58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['{\"activation_function\": \"relu\", \"batch_size\": 3072, \"learning_rate\": 0.007, \"num_layers\": 4}'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f97a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1eafb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99435d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae9fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53538279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "def A_sim(matrix_pi, stage):\n",
    "    N, K = matrix_pi.shape  # sample size and treatment options\n",
    "    if N <= 1 or K <= 1:\n",
    "        logger.error(\"Sample size or treatment options are insufficient! N: %d, K: %d\", N, K)\n",
    "        raise ValueError(\"Sample size or treatment options are insufficient!\")\n",
    "    if torch.any(matrix_pi < 0):\n",
    "        logger.error(\"Treatment probabilities should not be negative!\")\n",
    "        raise ValueError(\"Treatment probabilities should not be negative!\")\n",
    "\n",
    "    # Normalize probabilities to add up to 1 and simulate treatment A for each row\n",
    "    pis = matrix_pi.sum(dim=1, keepdim=True)\n",
    "    probs = matrix_pi / pis\n",
    "    A = torch.multinomial(probs, 1).squeeze()\n",
    "\n",
    "    if stage == 1:\n",
    "        col_names = ['pi_10', 'pi_11', 'pi_12']\n",
    "    else:\n",
    "        col_names = ['pi_20', 'pi_21', 'pi_22']\n",
    "    \n",
    "    probs_dict = {name: probs[:, idx] for idx, name in enumerate(col_names)}\n",
    "    \n",
    "    return {'A': A, 'probs': probs_dict}\n",
    "\n",
    "def M_propen(A, Xs, stage):\n",
    "    \"\"\"Estimate propensity scores using logistic or multinomial regression.\"\"\"\n",
    "    A = np.asarray(A).reshape(-1, 1)\n",
    "    if A.shape[1] != 1:\n",
    "        raise ValueError(\"Cannot handle multiple stages of treatments together!\")\n",
    "    if A.shape[0] != Xs.shape[0]:\n",
    "        print(\"A.shape, Xs.shape: \", A.shape, Xs.shape)\n",
    "        raise ValueError(\"A and Xs do not match in dimension!\")\n",
    "    if len(np.unique(A)) <= 1:\n",
    "        raise ValueError(\"Treatment options are insufficient!\")\n",
    "\n",
    "    # Handle multinomial case using Logistic Regression\n",
    "    encoder = OneHotEncoder(sparse_output=False)  # Updated parameter name\n",
    "    A_encoded = encoder.fit_transform(A)\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "    # Suppressing warnings from the solver, if not converged\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "        model.fit(Xs, A.ravel())\n",
    "\n",
    "    # Predicting probabilities\n",
    "    s_p = model.predict_proba(Xs)\n",
    "\n",
    "    if stage == 1:\n",
    "        col_names = ['pi_10', 'pi_11', 'pi_12']\n",
    "    else:\n",
    "        col_names = ['pi_20', 'pi_21', 'pi_22']\n",
    "    probs_df = pd.DataFrame(s_p, columns=col_names)\n",
    "\n",
    "    return probs_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80ac1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(replication_seed)\n",
    "sample_size = 10\n",
    "device = 'cpu'\n",
    "\n",
    "# Simulate baseline covariates\n",
    "O1 = torch.randn(5, sample_size, device=device)\n",
    "Z1 = torch.randn(sample_size, device=device)\n",
    "Z2 = torch.randn(sample_size, device=device)\n",
    "\n",
    "\n",
    "# Stage 1 data simulation\n",
    "x1, x2, x3, x4, x5 = O1[0], O1[1], O1[2], O1[3], O1[4]\n",
    "pi_10 = torch.ones(sample_size, device=device)\n",
    "pi_11 = torch.exp(0.5 - 0.5 * x3)\n",
    "pi_12 = torch.exp(0.5 * x4)\n",
    "matrix_pi1 = torch.stack((pi_10, pi_11, pi_12), dim=0).t()\n",
    "\n",
    "result1 = A_sim(matrix_pi1, stage=1)\n",
    "\n",
    "#     A1, probs1 = result1['A'], result1['probs']\n",
    "A1, r1 = result1['A'], result1['probs']\n",
    "# Propensity stage 1\n",
    "probs1 = M_propen(A1, O1[[2, 3]].t(), stage=1)  # multinomial logistic regression with X3, X4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "de17052e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pi_10</th>\n",
       "      <th>pi_11</th>\n",
       "      <th>pi_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.397228</td>\n",
       "      <td>0.494092</td>\n",
       "      <td>0.108680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333437</td>\n",
       "      <td>0.388866</td>\n",
       "      <td>0.277697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.078770</td>\n",
       "      <td>0.641926</td>\n",
       "      <td>0.279303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.478386</td>\n",
       "      <td>0.410891</td>\n",
       "      <td>0.110723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.077475</td>\n",
       "      <td>0.437941</td>\n",
       "      <td>0.484584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.058862</td>\n",
       "      <td>0.733756</td>\n",
       "      <td>0.207381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.378171</td>\n",
       "      <td>0.525913</td>\n",
       "      <td>0.095916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.447531</td>\n",
       "      <td>0.445077</td>\n",
       "      <td>0.107393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.473132</td>\n",
       "      <td>0.343207</td>\n",
       "      <td>0.183661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.276969</td>\n",
       "      <td>0.578345</td>\n",
       "      <td>0.144686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pi_10     pi_11     pi_12\n",
       "0  0.397228  0.494092  0.108680\n",
       "1  0.333437  0.388866  0.277697\n",
       "2  0.078770  0.641926  0.279303\n",
       "3  0.478386  0.410891  0.110723\n",
       "4  0.077475  0.437941  0.484584\n",
       "5  0.058862  0.733756  0.207381\n",
       "6  0.378171  0.525913  0.095916\n",
       "7  0.447531  0.445077  0.107393\n",
       "8  0.473132  0.343207  0.183661\n",
       "9  0.276969  0.578345  0.144686"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1accff06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pi_10</th>\n",
       "      <th>pi_11</th>\n",
       "      <th>pi_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.321233</td>\n",
       "      <td>0.455503</td>\n",
       "      <td>0.223264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.233064</td>\n",
       "      <td>0.334597</td>\n",
       "      <td>0.432340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.179841</td>\n",
       "      <td>0.566244</td>\n",
       "      <td>0.253915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.330334</td>\n",
       "      <td>0.404783</td>\n",
       "      <td>0.264883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147126</td>\n",
       "      <td>0.410865</td>\n",
       "      <td>0.442009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.175030</td>\n",
       "      <td>0.647368</td>\n",
       "      <td>0.177602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.325254</td>\n",
       "      <td>0.479938</td>\n",
       "      <td>0.194808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.329396</td>\n",
       "      <td>0.427222</td>\n",
       "      <td>0.243382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.279626</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.393454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.280332</td>\n",
       "      <td>0.494279</td>\n",
       "      <td>0.225388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pi_10     pi_11     pi_12\n",
       "0  0.321233  0.455503  0.223264\n",
       "1  0.233064  0.334597  0.432340\n",
       "2  0.179841  0.566244  0.253915\n",
       "3  0.330334  0.404783  0.264883\n",
       "4  0.147126  0.410865  0.442009\n",
       "5  0.175030  0.647368  0.177602\n",
       "6  0.325254  0.479938  0.194808\n",
       "7  0.329396  0.427222  0.243382\n",
       "8  0.279626  0.326921  0.393454\n",
       "9  0.280332  0.494279  0.225388"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c0afbbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 1, 1, 0, 2, 2, 2, 0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d68cc9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2479, -0.0342, -0.5946,  0.1033,  0.4231, -0.3447, -0.4391, -0.9040,\n",
       "         -0.1877,  0.3113],\n",
       "        [-0.9426,  0.0173, -1.1182,  1.5023, -0.8820, -0.3289, -0.8081, -1.2399,\n",
       "          1.4100,  0.2322]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O1[[2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00c72784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5906e-01, -8.7782e-01, -6.0655e-01,  1.6869e-01, -7.3726e-01,\n",
       "         -1.7103e+00,  1.7288e-01, -6.4159e-01, -4.9704e-01, -1.5568e+00],\n",
       "        [-1.5049e+00, -8.8236e-01,  1.6431e-03, -7.6861e-01,  3.0450e-01,\n",
       "          5.0576e-01, -8.9674e-01,  3.1831e-01, -1.5424e+00, -1.5294e+00],\n",
       "        [-2.4786e-01, -3.4243e-02, -5.9459e-01,  1.0329e-01,  4.2314e-01,\n",
       "         -3.4472e-01, -4.3906e-01, -9.0396e-01, -1.8769e-01,  3.1130e-01],\n",
       "        [-9.4263e-01,  1.7326e-02, -1.1182e+00,  1.5023e+00, -8.8204e-01,\n",
       "         -3.2889e-01, -8.0814e-01, -1.2399e+00,  1.4100e+00,  2.3216e-01],\n",
       "        [ 5.3532e-01, -8.3709e-02,  1.0224e-02, -1.4863e+00,  1.6502e+00,\n",
       "          9.7107e-01,  1.0964e-01,  7.8011e-01,  2.1684e+00, -4.5745e-02]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6d7ba3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O1[:,[2,3]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fb34842a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O1[[2, 3]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc3663dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2479, -0.0342, -0.5946,  0.1033,  0.4231, -0.3447, -0.4391, -0.9040,\n",
       "         -0.1877,  0.3113],\n",
       "        [-0.9426,  0.0173, -1.1182,  1.5023, -0.8820, -0.3289, -0.8081, -1.2399,\n",
       "          1.4100,  0.2322]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O1[[2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026411c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240dced3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240654da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae294d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ede23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0b1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3606581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34efae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afcc7d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting:  tao\n",
      "f_model:  tao\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c4b58d18e54555b9b795b9d08b1406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Replications_M1:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: nnet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGP:  tao\n",
      "Train model:  tao \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "\n"
     ]
    },
    {
     "ename": "RRuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRRuntimeError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/76/r06mvw257lbbll_cytzz3_3h0000gn/T/ipykernel_7796/2568503907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0mnetwork_parameters_surogate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f_model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tao'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m     V_replications, df, values_opt, losses_dict, epoch_num_model_lst = simulations(surrogate_num,\n\u001b[0m\u001b[1;32m   1360\u001b[0m                                                                                \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m                                                                                \u001b[0mnum_replications\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/76/r06mvw257lbbll_cytzz3_3h0000gn/T/ipykernel_7796/2568503907.py\u001b[0m in \u001b[0;36msimulations\u001b[0;34m(surrogate_num, sample_size, num_replications, V_replications, params)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;31m#  Estimate treatment regime: params['f_model']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f_model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tao'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mselect2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselects\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaptive_contrast_tao\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"contrast\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;31m# eval_DTR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mV_replications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_DTR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_replications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplication\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/76/r06mvw257lbbll_cytzz3_3h0000gn/T/ipykernel_7796/2568503907.py\u001b[0m in \u001b[0;36madaptive_contrast_tao\u001b[0;34m(all_data, contrast)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Call the R function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobalenv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_ACWL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg1_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mf_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/rpy2/robjects/functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         return (super(SignatureTranslatedFunction, self)\n\u001b[0m\u001b[1;32m    205\u001b[0m                 .__call__(*args, **kwargs))\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/rpy2/robjects/functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mnew_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy2rpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/rpy2/rinterface_lib/conversion.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_res_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# TODO: test cdata is of the expected CType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cdata_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/rpy2/rinterface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m             )\n\u001b[1;32m    816\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merror_occured\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rinterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geterrmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import norm\n",
    "from collections import Counter\n",
    "\n",
    "import pdb\n",
    "# # Set the seed for reproducibility\n",
    "# seed = 12345\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pip install rpy2\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import numpy2ri\n",
    "\n",
    "# Activate automatic conversion of numpy objects to R objects\n",
    "numpy2ri.activate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batches(N, batch_size, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, N, batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx+batch_size]\n",
    "        yield torch.tensor(batch_indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "def extract_and_prepare_data(A1_tensor_test, A2_tensor_test, A1, A2, d1_star, d2_star):\n",
    "\n",
    "    # Helper function to convert tensors to numpy arrays\n",
    "    def to_numpy(data):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return data.cpu().numpy() if data.is_cuda else data.numpy()\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            return data\n",
    "        else:\n",
    "            raise TypeError(\"The input must be a PyTorch tensor or a NumPy array\")\n",
    "\n",
    "    new_row = {\n",
    "        'Behavioral_A1': to_numpy(A1_tensor_test).tolist(),\n",
    "        'Behavioral_A2': to_numpy(A2_tensor_test).tolist(),\n",
    "        'Predicted_A1': to_numpy(A1).tolist(),\n",
    "        'Predicted_A2': to_numpy(A2).tolist(),\n",
    "        'Optimal_A1': to_numpy(d1_star).tolist(),\n",
    "        'Optimal_A2': to_numpy(d2_star).tolist()\n",
    "    }\n",
    "\n",
    "    return new_row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NNClass(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_networks, dropout_rate):\n",
    "        super(NNClass, self).__init__()\n",
    "        self.networks = nn.ModuleList()\n",
    "        for _ in range(num_networks):\n",
    "            network = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ELU(alpha=0.4),  # Using ELU instead of ReLU; best result 0.2, 0.13\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ELU(alpha=0.4),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_dim, output_dim),\n",
    "                nn.BatchNorm1d(output_dim),\n",
    "            )\n",
    "            self.networks.append(network)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for network in self.networks:\n",
    "            outputs.append(network(x))\n",
    "        return outputs\n",
    "\n",
    "    def he_initializer(self):\n",
    "        for network in self.networks:\n",
    "            for layer in network:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    nn.init.constant_(layer.bias, 0)  # Biases can be initialized to zero\n",
    "\n",
    "    def reset_weights(self):\n",
    "        for network in self.networks:\n",
    "            for layer in network:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.constant_(layer.weight, 0.1) # best 0.1\n",
    "                    nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## train V validation loss\n",
    "\n",
    "def plot_simulation_surLoss_losses_in_grid(selected_indices, losses_dict, cols=3):\n",
    "    # Calculate the number of rows needed based on the number of selected indices and desired number of columns\n",
    "    rows = len(selected_indices) // cols + (len(selected_indices) % cols > 0)\n",
    "\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))  # Adjust figure size as needed\n",
    "    fig.suptitle(f'Training and Validation Loss for Selected Simulations @ n_epoch = {n_epoch}')\n",
    "\n",
    "    # Flatten the axes array for easy indexing, in case of a single row or column\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        train_loss, val_loss = losses_dict[idx]\n",
    "\n",
    "        # Plot on the ith subplot\n",
    "        axes[i].plot(train_loss, label='Training')\n",
    "        axes[i].plot(val_loss, label='Validation')\n",
    "        axes[i].set_title(f'Simulation {idx}')\n",
    "        axes[i].set_xlabel('Epochs')\n",
    "        axes[i].set_ylabel('Loss')\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the suptitle\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def transform_Y(Y1, Y2):\n",
    "\n",
    "    # Identify the minimum value among Y1 and Y2, only if they are negative\n",
    "    min_negative_Y = min(min(Y1.min(), 0), min(Y2.min(), 0))\n",
    "\n",
    "    # If the minimum value is negative, adjust Y1 and Y2 by adding the absolute\n",
    "    # value of min_negative_Y plus 1 to ensure all values are non-negative\n",
    "    if min_negative_Y < 0:\n",
    "        Y1_trans = Y1 - min_negative_Y + 1\n",
    "        Y2_trans = Y2 - min_negative_Y + 1\n",
    "    else:\n",
    "        # If there are no negative values, no adjustment is needed\n",
    "        Y1_trans = Y1\n",
    "        Y2_trans = Y2\n",
    "\n",
    "    return Y1_trans, Y2_trans\n",
    "\n",
    "\n",
    "def A_sim(matrix_pi, stage):\n",
    "    N, K = matrix_pi.shape  # sample size and treatment options\n",
    "    if N <= 1 or K <= 1:\n",
    "        raise ValueError(\"Sample size or treatment options are insufficient!\")\n",
    "    if np.min(matrix_pi) < 0:\n",
    "        raise ValueError(\"Treatment probabilities should not be negative!\")\n",
    "\n",
    "    # Normalize probabilities to add up to 1 and simulate treatment A for each row\n",
    "    pis = matrix_pi.sum(axis=1)\n",
    "    probs = np.divide(matrix_pi, pis[:, np.newaxis])\n",
    "    A = np.array([np.random.choice(np.arange(K), p=probs[i,]) for i in range(N)])\n",
    "    if stage == 1:\n",
    "        col_names = ['pi_10', 'pi_11', 'pi_12']\n",
    "    else:\n",
    "        col_names = ['pi_20', 'pi_21', 'pi_22']\n",
    "    probs_df = pd.DataFrame(probs, columns=col_names)\n",
    "    return {'A': A, 'probs': probs_df}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(sample_size, setting, replication_seed):\n",
    "    np.random.seed(replication_seed)\n",
    "\n",
    "    print(\"DGP: \", setting)\n",
    "\n",
    "\n",
    "    if setting == 'linear':\n",
    "\n",
    "        # Generate data\n",
    "        O1, O2 = np.random.normal(0, 1, size=(sample_size, 2)), np.random.normal(0, 1, sample_size) # Combined Stage 1 Inputs O1 with 2 features\n",
    "\n",
    "\n",
    "        A1, A2 = np.random.randint(1, 4, size=sample_size), np.random.randint(1, 4, size=sample_size)\n",
    "        Z1, Z2 = np.random.normal(0, 0.5, sample_size), np.random.normal(0, 0.5, sample_size)\n",
    "\n",
    "        # Compute Y1 and Y2\n",
    "        Y1 = 15 + A1 + O1.sum(axis=1) + np.prod(O1, axis=1) + Z1\n",
    "        Y2 = 15 + O2 + A2 * (1 - O2 + A1 + O1.sum(axis=1)) + Z2\n",
    "\n",
    "        pi_value = np.full(sample_size, 1 / 3)  # Probability value when there are 3 treatments\n",
    "        pi_10 = pi_11 = pi_12 = pi_20 = pi_21 = pi_22 = pi_value\n",
    "\n",
    "        # Compute optimal policy decisions for 'linear', updated for combined O1 tensor\n",
    "        g1_opt = np.full(sample_size, 3)  # Assuming the optimal policy for stage 1 is to always choose action 3\n",
    "        g2_opt = np.where( (1 - O2 + g1_opt + O1.sum(axis=1)).astype(float) > 0, 3, 1)\n",
    "\n",
    "\n",
    "\n",
    "    elif setting == 'scheme_i':\n",
    "        # Generate data\n",
    "        O1 = np.random.normal(0, 1, (sample_size, 3))\n",
    "        Z1, Z2, A1, A2, O2 = [np.random.normal(0, 1, sample_size) for _ in range(2)] + \\\n",
    "                             [np.random.randint(1, 4, sample_size) for _ in range(2)] + \\\n",
    "                             [np.random.normal(0, 1, sample_size)]\n",
    "\n",
    "\n",
    "        # Compute Y1 using g(O1) and A1\n",
    "        Y1 = A1 * g(O1) + C1 + Z1\n",
    "\n",
    "        # Compute Y2 as a weighted sum of f_i(O1, A1) based on the value of A2\n",
    "        Y2 = sum(f_i(O1, A1, i) * (A2 == i) for i in range(1, 4)) + O2 * beta + C2 + Z2\n",
    "\n",
    "        # Probabilities for treatments, here assuming it's the same as linear case\n",
    "        pi_value = np.full(sample_size, 1 / 3)  # Equal probability for 3 treatments\n",
    "        pi_10 = pi_11 = pi_12 = pi_20 = pi_21 = pi_22 = pi_value\n",
    "\n",
    "\n",
    "    elif setting == 'tao':\n",
    "\n",
    "\n",
    "        # Simulate baseline covariates\n",
    "        N = sample_size\n",
    "        x1, x2, x3, x4, x5 = np.random.normal(size=(5, N))\n",
    "        O1 = np.column_stack((x1, x2, x3, x4, x5))\n",
    "\n",
    "        Z1 = np.random.normal(0, 1, sample_size)\n",
    "        Z2 = np.random.normal(0, 1, sample_size)\n",
    "\n",
    "        if noiseless:\n",
    "            Z1 = 0\n",
    "            Z2 = 0\n",
    "\n",
    "        # Stage 1 data simulation\n",
    "        pi_10 = np.ones(N)\n",
    "        pi_11 = np.exp(0.5 - 0.5 * x3)\n",
    "        pi_12 = np.exp(0.5 * x4)\n",
    "        matrix_pi1 = np.column_stack((pi_10, pi_11, pi_12))\n",
    "        result1 = A_sim(matrix_pi1, stage=1)\n",
    "        A1, probs1 = result1['A'], result1['probs']\n",
    "        A1 +=1\n",
    "\n",
    "\n",
    "        # Optimal g1.opt corrected\n",
    "        g1_opt = ((x1 > -1).astype(float) * ((x2 > -0.5).astype(float) + (x2 > 0.5).astype(float))).astype(float) + 1\n",
    "\n",
    "\n",
    "        # Stage 1 outcome R1\n",
    "\n",
    "        Y1 = np.exp(1.5 - np.abs(1.5 * x1 + 2) * (A1 - g1_opt)**2) + Z1\n",
    "\n",
    "        # Stage 2 data simulation\n",
    "        pi_20 = np.ones(N)\n",
    "        pi_21 = np.exp(0.2 * Y1 - 1)\n",
    "        pi_22 = np.exp(0.5 * x4)\n",
    "        matrix_pi2 = np.column_stack((pi_20, pi_21, pi_22))\n",
    "        result2 = A_sim(matrix_pi2, stage=2)\n",
    "        A2, probs2 = result2['A'], result2['probs']\n",
    "        A2 +=1\n",
    "\n",
    "        Y1_opt = np.exp(1.5) + Z1\n",
    "\n",
    "        g2_opt = (x3 > -1).astype(float) * ((Y1_opt > 0.5).astype(float) + (Y1_opt > 3).astype(float)) + 1\n",
    "\n",
    "\n",
    "\n",
    "        # Stage 2 outcome R2\n",
    "        Y2 = np.exp(1.26 - np.abs(1.5 * x3 - 2) * (A2 - g2_opt)**2) + Z2\n",
    "\n",
    "\n",
    "        # Extract propensity for both stages\n",
    "        pi_10, pi_11, pi_12 = probs1['pi_10'], probs1['pi_11'], probs1['pi_12']\n",
    "        pi_20, pi_21, pi_22 = probs2['pi_20'], probs2['pi_21'], probs2['pi_22']\n",
    "\n",
    "        # Dummy O2\n",
    "        O2 = np.zeros(sample_size)\n",
    "\n",
    "    Y1_trans, Y2_trans = transform_Y(Y1, Y2)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'dgp': setting,\n",
    "        'O1': torch.tensor(O1, dtype=torch.float32, device=device),\n",
    "        'A1': torch.tensor(A1, dtype=torch.float32, device=device),\n",
    "        'Z1': torch.tensor(Z1, dtype=torch.float32, device=device),\n",
    "        'Y1': torch.tensor(Y1, dtype=torch.float32, device=device),\n",
    "        'O2': torch.tensor(O2, dtype=torch.float32, device=device),\n",
    "        'A2': torch.tensor(A2, dtype=torch.float32, device=device),\n",
    "        'Z2': torch.tensor(Z2, dtype=torch.float32, device=device),\n",
    "        'Y2': torch.tensor(Y2, dtype=torch.float32, device=device),\n",
    "        'pi_10': torch.tensor(pi_10, dtype=torch.float32, device=device),\n",
    "        'pi_11': torch.tensor(pi_11, dtype=torch.float32, device=device),\n",
    "        'pi_12': torch.tensor(pi_12, dtype=torch.float32, device=device),\n",
    "        'pi_20': torch.tensor(pi_20, dtype=torch.float32, device=device),\n",
    "        'pi_21': torch.tensor(pi_21, dtype=torch.float32, device=device),\n",
    "        'pi_22': torch.tensor(pi_22, dtype=torch.float32, device=device),\n",
    "        'Y1_trans': torch.tensor(Y1_trans, dtype=torch.float32, device=device),\n",
    "        'Y2_trans': torch.tensor(Y2_trans, dtype=torch.float32, device=device),\n",
    "        'g1_opt': torch.tensor(g1_opt, dtype=torch.float32, device=device),\n",
    "        'g2_opt': torch.tensor(g2_opt, dtype=torch.float32, device=device),\n",
    "    }\n",
    "\n",
    "\n",
    "# Preprocess\n",
    "def preprocess_data(generated_data, sample_size, setting, replication_seed, run='train', training_validation_prop = 0.8):\n",
    "\n",
    "    # Extract data from the generated_data dictionary\n",
    "    O1_tensor, O2_tensor = [generated_data[key] for key in ['O1', 'O2']]\n",
    "    A1_tensor, A2_tensor = [generated_data[key] for key in ['A1', 'A2']]\n",
    "    g1_opt, g2_opt = [generated_data[key] for key in ['g1_opt', 'g2_opt']]\n",
    "\n",
    "\n",
    "    if run == 'test':\n",
    "        Y1_tensor, Y2_tensor = [generated_data[key] for key in ['Y1', 'Y2']]\n",
    "    else:\n",
    "        Y1_tensor, Y2_tensor = [generated_data[key] for key in ['Y1_trans', 'Y2_trans']]\n",
    "\n",
    "    if setting == 'tao':\n",
    "        Y1_tensor_opt = torch.exp(torch.tensor(1.5)) + generated_data['Z1']\n",
    "        if tree_type:\n",
    "            g1_opt = ((O1_tensor[:, 0] > -1).float() * ((O1_tensor[:, 1] > -0.5).float() + (O1_tensor[:, 1] > 0.5).float())) + 1\n",
    "\n",
    "            g2_opt = ((O1_tensor[:, 2] > -1).float() * ((Y1_tensor_opt > 0.5).float() + (Y1_tensor_opt > 3).float())) + 1\n",
    "        else:\n",
    "\n",
    "            g1_opt = ((O1_tensor[:, 0] > -0.5).float() * (1 + (O1_tensor[:, 0] - O1_tensor[:, 1] > 0).float())) + 1\n",
    "\n",
    "            g2_opt = ((O1_tensor[:, 2] > 0).float() + ((O1_tensor[:, 2] + Y1_tensor_opt > 2.5).float())) + 1\n",
    "\n",
    "        Y2_tensor = torch.exp(torch.tensor(1.26) - torch.abs(torch.tensor(1.5) * O1_tensor[:, 2] - 2) * (A2_tensor - g2_opt)**2) + generated_data[\"Z2\"]\n",
    "\n",
    "\n",
    "    # Probabilities\n",
    "    pi_tensors = [generated_data[key] for key in ['pi_10', 'pi_11', 'pi_12', 'pi_20', 'pi_21', 'pi_22']]\n",
    "    pi_tensor_stack = torch.stack(pi_tensors)\n",
    "\n",
    "    # Adjusting A1 and A2 indices\n",
    "    A1_indices = (A1_tensor - 1).long().unsqueeze(0)  # A1 actions, Subtract 1 to match index values (0, 1, 2)\n",
    "    A2_indices = (A2_tensor - 1 + 3).long().unsqueeze(0)   # A2 actions, Add +3 to match index values (3, 4, 5) for A2, with added dimension\n",
    "\n",
    "    # Gathering probabilities based on actions\n",
    "    # Since we're gathering along the first dimension, we need to use the dim=0 parameter in torch.gather\n",
    "    P_A1_given_H1_tensor = torch.gather(pi_tensor_stack, dim=0, index=A1_indices).squeeze(0)  # Remove the added dimension after gathering\n",
    "    P_A2_given_H2_tensor = torch.gather(pi_tensor_stack, dim=0, index=A2_indices).squeeze(0)  # Remove the added dimension after gathering\n",
    "\n",
    "    # Calculate Ci tensor\n",
    "    Ci_tensor = (Y1_tensor + Y2_tensor) / (P_A1_given_H1_tensor * P_A2_given_H2_tensor)\n",
    "\n",
    "    # Input preparation\n",
    "    input_stage1 = O1_tensor\n",
    "\n",
    "    if setting == 'tao':\n",
    "        input_stage2 = torch.cat([O1_tensor, A1_tensor.unsqueeze(1), Y1_tensor.unsqueeze(1)], dim=1)\n",
    "    else:\n",
    "        input_stage2 = torch.cat([O1_tensor, A1_tensor.unsqueeze(1), Y1_tensor.unsqueeze(1), O2_tensor.unsqueeze(1)], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    if run == 'test':\n",
    "        return input_stage1, input_stage2, Ci_tensor\n",
    "\n",
    "    # print(\"input_stage2: \", input_stage2[:5, :])\n",
    "\n",
    "    # Splitting data into training and validation sets\n",
    "    train_size = int(training_validation_prop * sample_size)\n",
    "    train_tensors = [tensor[:train_size] for tensor in [input_stage1, input_stage2, Ci_tensor, Y1_tensor, Y2_tensor, A1_tensor, A2_tensor]]\n",
    "    val_tensors = [tensor[train_size:] for tensor in [input_stage1, input_stage2, Ci_tensor, Y1_tensor, Y2_tensor, A1_tensor, A2_tensor]]\n",
    "\n",
    "    return tuple(train_tensors), tuple(val_tensors), tuple([input_stage1, input_stage2, Ci_tensor, Y1_tensor, Y2_tensor, A1_tensor, A2_tensor, pi_tensor_stack, g1_opt, g2_opt])\n",
    "\n",
    "\n",
    "def adaptive_contrast_tao(all_data, contrast):\n",
    "\n",
    "    train_input_stage1, train_input_stage2, train_Ci, train_Y1, train_Y2, train_A1, train_A2, pi_tensor_stack, g1_opt, g2_opt = all_data\n",
    "\n",
    "    A1 = train_A1.numpy()\n",
    "    probs1 = pi_tensor_stack.T[:, :3].numpy()\n",
    "\n",
    "    A2 = train_A2.numpy()\n",
    "    probs2 = pi_tensor_stack.T[:, 3:].numpy()\n",
    "\n",
    "    R1 = train_Y1.numpy()\n",
    "    R2 = train_Y2.numpy()\n",
    "\n",
    "    g1_opt = g1_opt.numpy()\n",
    "    g2_opt = g2_opt.numpy()\n",
    "\n",
    "\n",
    "\n",
    "    if setting == 'tao':\n",
    "\n",
    "        # Convert tensor to numpy if not already numpy array\n",
    "        train_input_np = train_input_stage1.numpy()\n",
    "\n",
    "        x1 = train_input_np[:, 0]\n",
    "        x2 = train_input_np[:, 1]\n",
    "        x3 = train_input_np[:, 2]\n",
    "        x4 = train_input_np[:, 3]\n",
    "        x5 = train_input_np[:, 4]\n",
    "\n",
    "\n",
    "        # Load the R script containing the function\n",
    "        ro.r('source(\"ACWL_tao.R\")')\n",
    "\n",
    "        # Call the R function\n",
    "        results = ro.globalenv['train_ACWL'](x1, x2, x3, x4, x5, A1, probs1, A2, probs2, R1, R2, g1_opt, g2_opt, contrast, method= f_model)\n",
    "\n",
    "\n",
    "    elif setting == 'linear':\n",
    "\n",
    "        # Convert tensor to numpy if not already numpy array\n",
    "        train_input_np = train_input_stage2.numpy()\n",
    "\n",
    "        x1 = train_input_np[:, 0]\n",
    "        x2 = train_input_np[:, 1]\n",
    "        O2 = train_input_np[:, 4]\n",
    "\n",
    "\n",
    "        # Load the R script containing the function\n",
    "        ro.r('source(\"ACWL_linear.R\")')\n",
    "\n",
    "        # Call the R function\n",
    "        results = ro.globalenv['train_ACWL'](x1, x2, O2, A1, probs1, A2, probs2, R1, R2, g1_opt, g2_opt, contrast, method= f_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Extract results\n",
    "    select2 = results.rx2('select2')[0]\n",
    "    select1 = results.rx2('select1')[0]\n",
    "    selects = results.rx2('selects')[0]\n",
    "\n",
    "    # TODO: FIX THESE TO GET EXACTLY SAME ACCURACY AS WE GET IN PYTHON\n",
    "    #print(f\"Train: Select1: {select1}, Select2: {select2}, Selects: {selects}\")\n",
    "\n",
    "    return select2, select1, selects\n",
    "\n",
    "\n",
    "\n",
    "def prepare_stage2_test_input(generated_test_data, A1):\n",
    "    # Extract tensors from the generated_test_data dictionary\n",
    "    O1_tensor_test, O2_tensor_test = [generated_test_data[key] for key in ['O1', 'O2']]\n",
    "    Z1_tensor_test = generated_test_data['Z1']\n",
    "\n",
    "    if generated_test_data['dgp'] == 'linear':\n",
    "        # Compute Y1_pred using the given formula, updated for combined O1 tensor\n",
    "        Y1_pred = 15 + A1 + O1_tensor_test.sum(axis=1) + torch.prod(O1_tensor_test, dim=1) + Z1_tensor_test\n",
    "\n",
    "    elif generated_test_data['dgp'] == 'scheme_i':\n",
    "        # Compute Y1_pred for scheme_i using g(O1) and given A1\n",
    "        Y1_pred = A1 * g(O1_tensor_test) + C1 + Z1_tensor_test\n",
    "\n",
    "    elif generated_test_data['dgp'] == 'tao':\n",
    "        # Determine g1_opt based on O1 features for the 'tao' setting\n",
    "        # Here we apply the conditions for the 'tao' setting to determine g1_opt\n",
    "\n",
    "\n",
    "        #         if tree_type:\n",
    "        #             g1_opt_conditions = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5) + (O1_tensor_test[:, 1] > 0.5))) + 1\n",
    "        #         else:\n",
    "        #             g1_opt_conditions = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0))) + 1\n",
    "\n",
    "\n",
    "        # Optimal policy conditions for Stage 1\n",
    "        if tree_type:\n",
    "            g1_opt_conditions = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5).float() + (O1_tensor_test[:, 1] > 0.5).float())) + 1\n",
    "        else:\n",
    "            g1_opt_conditions = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0).float())) + 1\n",
    "\n",
    "\n",
    "        # Assuming g1_opt_conditions gives the optimal action, we use it to compute Y1_pred\n",
    "        if noiseless:\n",
    "            Z1_tensor_test = 0\n",
    "\n",
    "        Y1_pred = torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (A1 - g1_opt_conditions)**2) + Z1_tensor_test\n",
    "\n",
    "    # Form the test input for stage 2 by concatenating the necessary tensors\n",
    "    if generated_test_data['dgp'] == 'tao':\n",
    "        test_input_stage2 = torch.cat([O1_tensor_test, A1.unsqueeze(1), Y1_pred.unsqueeze(1)], dim=1)\n",
    "        #print(\"Y1_pred [min, max, mean]: \", [torch.min(Y1_pred), torch.max(Y1_pred), torch.mean(Y1_pred)], torch.min(torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (A1 - g1_opt_conditions)**2)),  torch.min(Z1_tensor_test))\n",
    "        # Calculate the required quantities\n",
    "        Y1_stats = [torch.min(Y1_pred), torch.max(Y1_pred), torch.mean(Y1_pred)]\n",
    "        exp_calculation = torch.min(torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (A1 - g1_opt_conditions)**2))\n",
    "        #Z1_min = torch.min(Z1_tensor_test)\n",
    "\n",
    "        # Construct the message string\n",
    "        stats_message = f\"Y1_pred [min, max, mean]: {Y1_stats}\"\n",
    "\n",
    "        # Use tqdm.write() to print the stats\n",
    "        tqdm.write(stats_message)\n",
    "\n",
    "    else:\n",
    "        test_input_stage2 = torch.cat([O1_tensor_test, A1.unsqueeze(1), Y1_pred.unsqueeze(1), O2_tensor_test.unsqueeze(1)], dim=1)\n",
    "        Y1_stats = [torch.min(Y1_pred), torch.max(Y1_pred), torch.mean(Y1_pred)]\n",
    "        # Construct the message string\n",
    "        stats_message = f\"Y1_pred [min, max, mean]: {Y1_stats}\"\n",
    "        tqdm.write(stats_message)\n",
    "\n",
    "\n",
    "    return test_input_stage2, Y1_pred\n",
    "\n",
    "\n",
    "\n",
    "def prepare_Y2_pred(generated_test_data, A1, A2):\n",
    "    O1_tensor_test = generated_test_data['O1']\n",
    "    Z1_tensor_test = generated_test_data['Z1']\n",
    "    Z2_tensor_test = generated_test_data['Z2']  # Assuming Z2 is a key in your dictionary\n",
    "\n",
    "    if generated_test_data['dgp'] == 'linear':\n",
    "        O2_tensor_test = generated_test_data['O2']\n",
    "        # Compute Y2_pred for the 'linear' setting, updated for combined O1 tensor\n",
    "        Y2_pred = 15 + O2_tensor_test + A2 * (1 - O2_tensor_test + A1 + O1_tensor_test.sum(axis=1)) + Z2_tensor_test\n",
    "\n",
    "    elif generated_test_data['dgp'] == 'scheme_i':\n",
    "\n",
    "        # Compute Y2_pred for scheme_i\n",
    "        Y2_pred = sum(f_i(O1_tensor_test, A1, i) * (A2 == i) for i in range(1, 4)) + \\\n",
    "                  generated_test_data['O2'] * beta + C2 + Z2_tensor_test\n",
    "\n",
    "\n",
    "    elif generated_test_data['dgp'] == 'tao':\n",
    "        #         # Determine g2_opt based on O1 features for the 'tao' setting\n",
    "        #         if tree_type:\n",
    "        #             g2_opt_conditions = ((O1_tensor_test[:, 2] > -1).float() * ((generated_test_data['Y1'] > 0.5) + (generated_test_data['Y1'] > 3))) + 1\n",
    "        #         else:\n",
    "        #             # Assuming R1 is represented by Y1, which is already computed and passed as A1\n",
    "        #             g2_opt_conditions = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + generated_test_data['Y1'] > 2.5).float())) + 1\n",
    "\n",
    "        if tree_type:\n",
    "            #g1_opt_conditions = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5).float() + (O1_tensor_test[:, 1] > 0.5).float())) + 1\n",
    "            # Y1_pred = torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (d1_star - g1_opt_conditions)**2) #+ Z1_tensor_test\n",
    "            Y1_pred = torch.exp(torch.tensor(1.5)) + Z1_tensor_test\n",
    "\n",
    "            g2_opt_conditions = ((O1_tensor_test[:, 2] > -1).float() * ((Y1_pred > 0.5).float() + (Y1_pred > 3).float())) + 1\n",
    "\n",
    "        else:\n",
    "            # g1_opt_conditions = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0).float())) + 1\n",
    "            # Y1_pred = torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (d1_star - g1_opt_conditions)**2) #+ Z1_tensor_test\n",
    "            Y1_pred = torch.exp(torch.tensor(1.5)) #torch.exp(1.5) #+ Z1_tensor_test\n",
    "\n",
    "            g2_opt_conditions = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + Y1_pred > 2.5).float())) + 1\n",
    "\n",
    "\n",
    "        # Assuming g2_opt_conditions gives the optimal action, we use it to compute Y2_pred\n",
    "        if noiseless:\n",
    "            Z2_tensor_test = 0\n",
    "\n",
    "        Y2_pred = torch.exp(1.26 - torch.abs(1.5 * O1_tensor_test[:, 2] - 2) * (A2 - g2_opt_conditions)**2) + Z2_tensor_test\n",
    "\n",
    "    # print(\"Y2_pred [min, max, mean]: \", [torch.min(Y2_pred), torch.max(Y2_pred), torch.mean(Y2_pred)] )\n",
    "    stats_message = f\"Y2_pred [min, max, mean]: [{torch.min(Y2_pred)}, {torch.max(Y2_pred)}, {torch.mean(Y2_pred)}]\"\n",
    "    tqdm.write(stats_message)\n",
    "\n",
    "    return Y2_pred\n",
    "\n",
    "\n",
    "\n",
    "# def compute_optimal_policy(generated_test_data, A1, A2):\n",
    "def compute_optimal_policy(generated_test_data):\n",
    "    O1_tensor_test = generated_test_data['O1']\n",
    "    Z1_tensor_test = generated_test_data['Z1']\n",
    "\n",
    "    if generated_test_data['dgp'] == 'linear':\n",
    "        O2_tensor_test = generated_test_data['O2']\n",
    "        # Compute optimal policy decisions for 'linear', updated for combined O1 tensor\n",
    "        d1_star = torch.full_like(Z1_tensor_test, 3, dtype=torch.float)  # Assuming the optimal policy for stage 1 is to always choose action 3\n",
    "        d2_star = torch.where(1 - O2_tensor_test + d1_star + O1_tensor_test.sum(axis=1) > 0, torch.tensor(3.0), torch.tensor(1.0))\n",
    "\n",
    "\n",
    "    elif generated_test_data['dgp'] == 'scheme_i':\n",
    "\n",
    "        # Compute f_i for each action j and for each i\n",
    "        # and then find the maximum f_i for each j\n",
    "        f_max_for_each_j = torch.stack([\n",
    "            torch.stack([f_i(O1_tensor_test, j, i) for i in range(1, 4)], dim=1).max(dim=1)[0]\n",
    "            for j in range(1, 4)\n",
    "        ], dim=1)\n",
    "\n",
    "        # Compute h_j for each action j\n",
    "        h_j_values = f_max_for_each_j + torch.arange(1, 4, dtype=torch.float32) * g(O1_tensor_test).unsqueeze(1)\n",
    "\n",
    "        # Determine the action that maximizes h_j for each observation\n",
    "        d1_star = torch.argmax(h_j_values, dim=1) + 1  # +1 because actions are 1-indexed\n",
    "\n",
    "        # Compute d2_star, similar to d1_star but based on f_i values directly considering A1\n",
    "        f_i_values = torch.stack([f_i(O1_tensor_test, generated_test_data['A1'], i) for i in range(1, 4)], dim=1)\n",
    "        d2_star = torch.argmax(f_i_values, dim=1) + 1  # +1 because actions are 1-indexed\n",
    "\n",
    "    elif generated_test_data['dgp'] == 'tao':\n",
    "        # Determine optimal policies based on O1 features for the 'tao' setting\n",
    "        #         if tree_type:\n",
    "        #             d1_star = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5) + (O1_tensor_test[:, 1] > 0.5))) + 1\n",
    "        #             d2_star = ((O1_tensor_test[:, 2] > -1).float() * ((generated_test_data['Y1'] > 0.5) + (generated_test_data['Y1'] > 3))) + 1\n",
    "        #         else:\n",
    "        #             d1_star = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0))) + 1\n",
    "        #             d2_star = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + generated_test_data['Y1'] > 2.5).float())) + 1\n",
    "\n",
    "        Y1_pred = torch.exp(torch.tensor(1.5)) + Z1_tensor_test\n",
    "        if tree_type:\n",
    "            d1_star = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5).float() + (O1_tensor_test[:, 1] > 0.5).float())) + 1\n",
    "\n",
    "            #g1_opt_conditions = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5).float() + (O1_tensor_test[:, 1] > 0.5).float())) + 1\n",
    "            # Y1_pred = torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (d1_star - g1_opt_conditions)**2) #+ Z1_tensor_test\n",
    "\n",
    "            #Y1_pred = torch.full_like(generated_test_data['Y1'], torch.exp(torch.tensor(1.5)))\n",
    "\n",
    "            d2_star = ((O1_tensor_test[:, 2] > -1).float() * ((Y1_pred > 0.5).float() + (Y1_pred > 3).float())) + 1\n",
    "        else:\n",
    "\n",
    "            d1_star = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0).float())) + 1\n",
    "            # g1_opt_conditions = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0).float())) + 1\n",
    "            # Y1_pred = torch.exp(1.5 - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (d1_star - g1_opt_conditions)**2) #+ Z1_tensor_test\n",
    "\n",
    "            d2_star = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + Y1_pred > 2.5).float())) + 1\n",
    "\n",
    "    return d1_star, d2_star\n",
    "\n",
    "\n",
    "\n",
    "def calculate_optimal_policy_values(d1_star, d2_star, generated_test_data):\n",
    "\n",
    "    # Extract necessary tensors from the generated_test_data dictionary\n",
    "    O1_tensor_test, O2_tensor_test, Z1_tensor_test, Z2_tensor_test = [generated_test_data[key] for key in ['O1', 'O2', 'Z1', 'Z2']]\n",
    "\n",
    "    # For the 'tao' setting, we don't use O2_tensor_test as O2 is not generated in this setting\n",
    "    if generated_test_data['dgp'] == 'linear':\n",
    "        Y1_test_opt = 15 + d1_star + O1_tensor_test.sum(axis=1) + torch.prod(O1_tensor_test, dim=1) + Z1_tensor_test\n",
    "        Y2_test_opt = 15 + O2_tensor_test + d2_star * (1 - O2_tensor_test + d1_star + O1_tensor_test.sum(axis=1)) + Z2_tensor_test\n",
    "\n",
    "    elif generated_test_data['dgp'] == 'scheme_i':\n",
    "\n",
    "        # Compute the optimal Y1_test and Y2_test values using the optimal policies d1_star and d2_star\n",
    "        # For Y1, use the g function and add C1, Z1\n",
    "        Y1_test_opt = d1_star * g(O1_tensor_test) + C1 + Z1_tensor_test\n",
    "\n",
    "        # For Y2, sum the f_i for the d2_star action, add O2 times beta, C2, and Z2\n",
    "        Y2_test_opt = f_i(O1_tensor_test, d1_star, d2_star) + O2_tensor_test * beta + C2 + Z2_tensor_test\n",
    "\n",
    "\n",
    "    elif generated_test_data['dgp'] == 'tao':\n",
    "        # Determine g1_opt and g2_opt based on O1 features for the 'tao' setting\n",
    "        #         if tree_type:\n",
    "        #             g1_opt_conditions = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5) + (O1_tensor_test[:, 1] > 0.5))) + 1\n",
    "        #             g2_opt_conditions = ((O1_tensor_test[:, 2] > -1).float() * ((generated_test_data['Y1'] > 0.5) + (generated_test_data['Y1'] > 3))) + 1\n",
    "        #         else:\n",
    "        #             g1_opt_conditions = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0))) + 1\n",
    "        #             g2_opt_conditions = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + generated_test_data['Y1'] > 2.5).float())) + 1\n",
    "\n",
    "        if tree_type:\n",
    "            g1_opt_conditions = ((O1_tensor_test[:, 0] > -1).float() * ((O1_tensor_test[:, 1] > -0.5).float() + (O1_tensor_test[:, 1] > 0.5).float())) + 1\n",
    "        else:\n",
    "            g1_opt_conditions = ((O1_tensor_test[:, 0] > -0.5).float() * (1 + (O1_tensor_test[:, 0] - O1_tensor_test[:, 1] > 0).float())) + 1\n",
    "\n",
    "\n",
    "        # Calculate Y1_test_opt and Y2_test_opt using the determined g1_opt and g2_opt\n",
    "        Y1_test_opt = torch.exp(torch.tensor(1.5) - torch.abs(1.5 * O1_tensor_test[:, 0] + 2) * (d1_star - g1_opt_conditions)**2) + Z1_tensor_test\n",
    "\n",
    "        if tree_type:\n",
    "            g2_opt_conditions = ((O1_tensor_test[:, 2] > -1).float() * ((Y1_test_opt > 0.5).float() + (Y1_test_opt > 3).float())) + 1\n",
    "        else:\n",
    "            g2_opt_conditions = ((O1_tensor_test[:, 2] > 0).float() + ((O1_tensor_test[:, 2] + Y1_test_opt > 2.5).float())) + 1\n",
    "\n",
    "        Y2_test_opt = torch.exp(torch.tensor(1.26) - torch.abs(1.5 * O1_tensor_test[:, 2] - 2) * (d2_star - g2_opt_conditions)**2) + Z2_tensor_test\n",
    "\n",
    "        # the following is simplified work; above is just for clarity\n",
    "        # Y1_test_opt = torch.exp(torch.tensor(1.5)) + Z1_tensor_test\n",
    "        # Y2_test_opt = torch.exp(torch.tensor(1.26)) + Z2_tensor_test\n",
    "        test1 = torch.abs(1.5 * O1_tensor_test[:, 2] - 2) * (d2_star - g2_opt_conditions)**2\n",
    "\n",
    "    # Aggregate the values into a tuple\n",
    "    values_opt = (d1_star, O1_tensor_test, Z1_tensor_test, d2_star, Z2_tensor_test)\n",
    "\n",
    "    return Y1_test_opt, Y2_test_opt, values_opt\n",
    "\n",
    "\n",
    "\n",
    "def calculate_policy_values(d1_star, d2_star, generated_test_data, Y1_pred, Y2_pred, V_replications):\n",
    "    # Optimal policy value calculation\n",
    "    Y1_test_opt, Y2_test_opt, values_opt = calculate_optimal_policy_values(d1_star, d2_star, generated_test_data)\n",
    "    V_d1_d2_opt = torch.mean(Y1_test_opt + Y2_test_opt).cpu().item()  # Calculate the mean value and convert to Python scalar\n",
    "    V_replications[\"V_replications_M1_optimal\"].append(V_d1_d2_opt)  # Append to the list for optimal policy values\n",
    "\n",
    "    # Behavioral policy value calculation\n",
    "    V_d1_d2 = torch.mean(generated_test_data['Y1'] + generated_test_data['Y2']).cpu().item()  # Calculate the mean value and convert to Python scalar\n",
    "    V_replications[\"V_replications_M1_behavioral\"].append(V_d1_d2)  # Append to the list for behavioral policy values\n",
    "\n",
    "    # Current approach value calculation\n",
    "    V_replications[\"V_replications_M1_pred\"].append(torch.mean(Y1_pred + Y2_pred).item())  # Append the mean value as a Python scalar to the list for current approach values\n",
    "\n",
    "    return V_replications, values_opt\n",
    "\n",
    "\n",
    "def compute_test_outputs(nn, test_input, A_tensor, device, params, is_stage1=True):\n",
    "    with torch.no_grad():\n",
    "        if params['f_model'] == \"surr_opt\":\n",
    "            # Perform the forward pass\n",
    "            test_outputs_i = nn(test_input)\n",
    "\n",
    "            # Directly stack the required outputs and perform computations in a single step\n",
    "            test_outputs = torch.stack(test_outputs_i[:2], dim=1).squeeze()\n",
    "\n",
    "            # Compute treatment assignments directly without intermediate variables\n",
    "            test_outputs = torch.stack([\n",
    "                torch.zeros_like(test_outputs[:, 0]),\n",
    "                -test_outputs[:, 0],\n",
    "                -test_outputs[:, 1]\n",
    "            ], dim=1)\n",
    "        else:\n",
    "            # Modify input for each action and perform a forward pass\n",
    "            input_tests = [\n",
    "                torch.cat((test_input, torch.full_like(A_tensor, i).unsqueeze(-1)), dim=1).to(device)\n",
    "                for i in range(1, 4)  # Assuming there are 3 actions\n",
    "            ]\n",
    "\n",
    "            # Forward pass for each modified input and stack the results\n",
    "            test_outputs = torch.stack([\n",
    "                nn(input_stage)[0] for input_stage in input_tests\n",
    "            ], dim=1)\n",
    "\n",
    "    # Determine the optimal action based on the computed outputs\n",
    "    optimal_actions = torch.argmax(test_outputs, dim=1) + 1\n",
    "    return optimal_actions.squeeze().to(device), test_outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_DTR(sample_size, V_replications, num_replications, nn_stage1, nn_stage2, df, params):\n",
    "\n",
    "\n",
    "    # Calculate V using the best model parameters on the test data # Calculate V using the best model parameters on the test data\n",
    "    generated_test_data = generate_data(sample_size, params['setting'], replication_seed = num_replications)\n",
    "    # generated_test_data = generate_data_test(sample_size, params['setting'], replication_seed = num_replications)\n",
    "\n",
    "    # Preprocess data\n",
    "    test_input_stage1, test_input_stage2, Ci_tensor = preprocess_data(generated_test_data, sample_size, params['setting'], replication_seed=num_replications, run='test')\n",
    "    # test_input_stage1, test_input_stage2, Ci_tensor = preprocess_data_test(generated_test_data, params['setting'], replication_seed=num_replications, run='test')\n",
    "\n",
    "    A1_tensor_test, A2_tensor_test = [generated_test_data[key] for key in ['A1', 'A2']]\n",
    "\n",
    "\n",
    "    # optimal policy\n",
    "    d1_star, d2_star =  compute_optimal_policy(generated_test_data)\n",
    "\n",
    "\n",
    "    # Calculate test outputs for all networks in stage 1\n",
    "    # Perform forward pass\n",
    "\n",
    "    if params['f_model']!=\"tao\":\n",
    "        nn_stage1 = initialize_nn(params, 1)\n",
    "        nn_stage2 = initialize_nn(params, 2)\n",
    "\n",
    "        # Load the best model parameters for Stage 1 and Stage 2 during validation\n",
    "        if params['f_model']==\"surr_opt\":\n",
    "            nn_stage1.load_state_dict(torch.load(f'best_model_stage_surr_1_{sample_size}.pt'))\n",
    "            nn_stage2.load_state_dict(torch.load(f'best_model_stage_surr_2_{sample_size}.pt'))\n",
    "        elif params['f_model']==\"DQlearning\":\n",
    "            nn_stage1.load_state_dict(torch.load(f'best_model_stage_Q_1_{sample_size}.pt'))\n",
    "            nn_stage2.load_state_dict(torch.load(f'best_model_stage_Q_2_{sample_size}.pt'))\n",
    "\n",
    "        nn_stage1.eval()\n",
    "        nn_stage2.eval()\n",
    "\n",
    "        A1, test_outputs_stage1 = compute_test_outputs(nn_stage1, test_input_stage1, A1_tensor_test, device, params, is_stage1=True)\n",
    "        test_input_stage2, Y1_pred = prepare_stage2_test_input(generated_test_data, A1)\n",
    "\n",
    "        # Calculate test outputs for all networks in stage 2\n",
    "        A2, test_outputs_stage2 = compute_test_outputs(nn_stage2, test_input_stage2, A2_tensor_test, device, params, is_stage1=False)\n",
    "        Y2_pred =  prepare_Y2_pred(generated_test_data, A1, A2)\n",
    "    else:\n",
    "\n",
    "\n",
    "        if setting == 'tao':\n",
    "\n",
    "            test_input_np = test_input_stage1.numpy()\n",
    "            x1 = test_input_np[:, 0]\n",
    "            x2 = test_input_np[:, 1]\n",
    "            x3 = test_input_np[:, 2]\n",
    "            x4 = test_input_np[:, 3]\n",
    "            x5 = test_input_np[:, 4]\n",
    "\n",
    "\n",
    "            # Load the R script containing the function\n",
    "            ro.r('source(\"ACWL_tao.R\")')\n",
    "\n",
    "            # Call the R function\n",
    "            results = ro.globalenv['test_ACWL'](x1, x2, x3, x4, x5, d1_star.numpy(), d2_star.numpy(), noiseless, method= f_model)\n",
    "\n",
    "\n",
    "        elif setting == 'linear':\n",
    "\n",
    "            # Convert tensor to numpy if not already numpy array\n",
    "            test_input_np = test_input_stage2.numpy()\n",
    "\n",
    "            x1 = test_input_np[:, 0]\n",
    "            x2 = test_input_np[:, 1]\n",
    "            O2 = test_input_np[:, 4]\n",
    "\n",
    "\n",
    "            # Load the R script containing the function\n",
    "            ro.r('source(\"ACWL_linear.R\")')\n",
    "\n",
    "            # Call the R function\n",
    "            results = ro.globalenv['test_ACWL'](x1, x2, O2, d1_star.numpy(), d2_star.numpy(), noiseless, method= f_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Extract results\n",
    "\n",
    "        select2_test = results.rx2('select2')[0]\n",
    "        select1_test = results.rx2('select1')[0]\n",
    "        selects_test = results.rx2('selects')[0]\n",
    "\n",
    "        # TODO: FIX THESE TO GET EXACTLY SAME ACCURACY AS WE GET IN PYTHON\n",
    "        print(f\"TEST: Select1: {select1_test}, Select2: {select2_test}, Selects: {selects_test}\")\n",
    "\n",
    "\n",
    "        # Extracting each component of the results and convert them to tensors\n",
    "        Y1_pred_R = torch.tensor(np.array(results.rx2('R1.a1')), dtype=torch.float32)\n",
    "        Y2_pred_R = torch.tensor(np.array(results.rx2('R2.a1')), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: FIX THESE TO GET EXACTLY SAME ACCURACY AS WE GET IN PYTHON\n",
    "\n",
    "        Y1_stats_R = [torch.min(Y1_pred_R), torch.max(Y1_pred_R), torch.mean(Y1_pred_R)]\n",
    "        message = f\"Y1_pred_R [min, max, mean]: {Y1_stats_R}\"\n",
    "        tqdm.write(message)\n",
    "        message = f\"Y2_pred_R [min, max, mean]: [{torch.min(Y2_pred_R)}, {torch.max(Y2_pred_R)}, {torch.mean(Y2_pred_R)}]\"\n",
    "        tqdm.write(message)\n",
    "\n",
    "\n",
    "        # torch.mean(Y1_pred + Y2_pred): 4.660262107849121\n",
    "        message = f'torch.mean(Y1_pred_R + Y2_pred_R): {torch.mean(Y1_pred_R + Y2_pred_R)} \\n'\n",
    "        tqdm.write(message)\n",
    "\n",
    "\n",
    "\n",
    "        A1 = torch.tensor(np.array(results.rx2('g1.a1')), dtype=torch.float32)\n",
    "        A2 = torch.tensor(np.array(results.rx2('g2.a1')), dtype=torch.float32)\n",
    "\n",
    "        test_input_stage2, Y1_pred = prepare_stage2_test_input(generated_test_data, A1)\n",
    "        Y2_pred =  prepare_Y2_pred(generated_test_data, A1, A2)\n",
    "\n",
    "\n",
    "\n",
    "    # optimal policy\n",
    "    # d1_star, d2_star =  compute_optimal_policy(generated_test_data, A1, A2)\n",
    "\n",
    "    # Append to DataFrame\n",
    "    new_row = {\n",
    "        'Behavioral_A1': A1_tensor_test.cpu().numpy().tolist(),\n",
    "        'Behavioral_A2': A2_tensor_test.cpu().numpy().tolist(),\n",
    "        'Predicted_A1': A1.cpu().numpy().tolist(),\n",
    "        'Predicted_A2':  A2.cpu().numpy().tolist(),\n",
    "        'Optimal_A1': d1_star.cpu().numpy().tolist(),\n",
    "        'Optimal_A2': d2_star.cpu().numpy().tolist()\n",
    "        }\n",
    "\n",
    "    # new_row = extract_and_prepare_data(A1_tensor_test, A2_tensor_test, A1, A2, d1_star, d2_star)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    message = f'torch.mean(Y1_pred + Y2_pred): {torch.mean(Y1_pred + Y2_pred)} \\n\\n'\n",
    "    #message = f'torch.mean(Y1_pred + Y2_pred): {torch.mean(Y1_pred + Y2_pred).item() if isinstance(Y1_pred, torch.Tensor) else np.mean(Y1_pred + Y2_pred)}'\n",
    "    tqdm.write(message)\n",
    "\n",
    "    V_replications, values_opt = calculate_policy_values(d1_star, d2_star, generated_test_data, Y1_pred, Y2_pred, V_replications)\n",
    "\n",
    "\n",
    "    return V_replications, df, values_opt\n",
    "\n",
    "\n",
    "def simulations(surrogate_num, sample_size, num_replications, V_replications, params):\n",
    "    columns = ['Behavioral_A1', 'Behavioral_A2', 'Predicted_A1', 'Predicted_A2', 'Optimal_A1', 'Optimal_A2']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    losses_dict = {}\n",
    "    epoch_num_model_lst = []\n",
    "\n",
    "    for replication in tqdm(range(num_replications), desc=\"Replications_M1\"):\n",
    "\n",
    "        # Generate data\n",
    "        generated_data = generate_data(sample_size, params['setting'],replication_seed=replication)\n",
    "        # Preprocess data (moved to GPU)\n",
    "        tuple_train, tuple_Val, all_data = preprocess_data(generated_data, sample_size, params['setting'], replication_seed=replication)\n",
    "\n",
    "        #  Estimate treatment regime: params['f_model']\n",
    "        if params['f_model'] == 'tao':\n",
    "            (select2, select1, selects) = adaptive_contrast_tao(all_data, params[\"contrast\"])\n",
    "            # eval_DTR\n",
    "            V_replications, df, values_opt = eval_DTR(sample_size, V_replications, replication, None, None, df, params)\n",
    "\n",
    "\n",
    "        elif params['f_model'] == 'DQlearning':\n",
    "            (nn_stage1, nn_stage2, trn_val_loss_tpl, epoch_num_model_1, epoch_num_model_2) = DQlearning(sample_size, tuple_train, tuple_Val, params)\n",
    "            epoch_num_model_lst.append([epoch_num_model_1, epoch_num_model_2])\n",
    "            losses_dict[replication] = trn_val_loss_tpl\n",
    "            # eval_DTR\n",
    "            V_replications, df, values_opt = eval_DTR(sample_size, V_replications, replication, nn_stage1, nn_stage2, df, params )\n",
    "\n",
    "        else:\n",
    "            # surr_opt\n",
    "            nn_stage1, nn_stage2, trn_val_loss_tpl, epoch_num_model = surr_opt(sample_size, tuple_train, tuple_Val, surrogate_num, params)\n",
    "            epoch_num_model_lst.append(epoch_num_model)\n",
    "            losses_dict[replication] = trn_val_loss_tpl\n",
    "            # eval_DTR\n",
    "            V_replications, df, values_opt = eval_DTR(sample_size, V_replications, replication, nn_stage1, nn_stage2, df, params )\n",
    "\n",
    "    return V_replications, df, values_opt, losses_dict, epoch_num_model_lst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# sample_size = 1000  # 500, 1000 are the cases to check\n",
    "# batch_prop = 0.2 #0.07, 0.2\n",
    "# if sample_size < 500:\n",
    "#     batch_prop = 0.5\n",
    "\n",
    "# training_validation_prop = 0.5 #0.95 #0.01\n",
    "\n",
    "\n",
    "# # Prompt user for the number of replications\n",
    "# num_replications = 4\n",
    "\n",
    "# # Prompt user for the setting\n",
    "# setting = 'tao' # 'linear', 'tao', 'scheme_i'\n",
    "\n",
    "# noiseless = True # True False\n",
    "\n",
    "\n",
    "\n",
    "# if setting == 'tao':\n",
    "#     tree_type =  True # True False\n",
    "\n",
    "# # Prompt user for the model type\n",
    "# f_model = 'tao' # (linear, 'tao', 'DQlearning', 'surr_opt'): \" tao => adaptive_contrast_tao) # Note for linear linear run separate R code\n",
    "\n",
    "# contrast = 1\n",
    "\n",
    "# surrogate_num = 1 #1- old multiplicative one  2- new one\n",
    "\n",
    "# option_sur = 1 # if surrogate_num = 1 then from 1-5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "\n",
    "\n",
    "# # Constants from scheme_i\n",
    "# C1 = C2 = 3\n",
    "# beta = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # #BEST so far for tao's 15000\n",
    "\n",
    "# network_parameters_surogate = {\n",
    "#   'setting': setting,\n",
    "#   'n_epoch': 60, #250\n",
    "#   'num_networks': 2,\n",
    "#   'input_dim_stage1': 2,\n",
    "#   'output_dim_stage1': 1,\n",
    "#   'input_dim_stage2': 5,\n",
    "#   'output_dim_stage2': 1,\n",
    "#   'optimizer_betas': (0.9, 0.999),\n",
    "#   'optimizer_eps': 1e-08,\n",
    "#   'scheduler_step_size': 30,\n",
    "#   'scheduler_gamma': 0.8,\n",
    "#   'hidden_dim_stage1': 10, #20\n",
    "#   'hidden_dim_stage2': 10, #20\n",
    "#   'dropout_rate': 0.0, #0.3, 0.43\n",
    "#   'optimizer_lr': 0.07, # 0.07, 0.007\n",
    "#   'optimizer_weight_decay': 0.001,\n",
    "#   'batch_size': math.ceil(batch_prop*sample_size), #int(0.038*sample_size),\n",
    "#   'f_model': 'surr_opt',\n",
    "#   'option_sur': option_sur, # if surrogate_num = 1 then 5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "#   'contrast': contrast\n",
    "\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # input_stage2 = [O1, A1, Y1, O2]\n",
    "\n",
    "# if setting =='tao':\n",
    "#     network_parameters_surogate['input_dim_stage1'] = 5\n",
    "#     network_parameters_surogate['input_dim_stage2'] = 7\n",
    "\n",
    "\n",
    "# network_parameters_surogate['option_sur'] = 2 # symmetric\n",
    "# n_epoch = network_parameters_surogate['n_epoch']\n",
    "\n",
    "# # Set the device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Lists to store replication results\n",
    "# V_replications_M1_behavioral = []\n",
    "# V_replications_M1_pred = []\n",
    "# V_replications_M1_optimal = []\n",
    "\n",
    "# V_replications = {\"V_replications_M1_behavioral\": V_replications_M1_behavioral, \"V_replications_M1_pred\": V_replications_M1_pred, \"V_replications_M1_optimal\": V_replications_M1_optimal}\n",
    "\n",
    "\n",
    "# print('Setting: ' , setting)\n",
    "# print(\"f_model: \", f_model)\n",
    "\n",
    "\n",
    "# network_parameters_surogate['f_model'] = 'tao'\n",
    "# V_replications, df, values_opt, losses_dict, epoch_num_model_lst = simulations(surrogate_num,\n",
    "#                                                                            sample_size,\n",
    "#                                                                            num_replications,\n",
    "#                                                                            V_replications,\n",
    "#                                                                            network_parameters_surogate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "sample_size = 1000  # 500, 1000 are the cases to check\n",
    "batch_prop = 0.2 #0.07, 0.2\n",
    "if sample_size < 500:\n",
    "    batch_prop = 0.5\n",
    "\n",
    "training_validation_prop = 0.5 #0.95 #0.01\n",
    "\n",
    "\n",
    "# Prompt user for the number of replications\n",
    "num_replications = 9\n",
    "\n",
    "# Prompt user for the setting\n",
    "setting = 'tao' # 'linear', 'tao', 'scheme_i'\n",
    "\n",
    "noiseless = True # True False\n",
    "\n",
    "\n",
    "\n",
    "# Prompt user for the scheme if setting is 'scheme_i'\n",
    "if setting != 'tao':\n",
    "    scheme = 1 # scheme number - (1, 2, or 3)\n",
    "\n",
    "# If the setting is 'tao', ask for tree_type\n",
    "\n",
    "if setting == 'tao':\n",
    "    tree_type =  True # True False\n",
    "\n",
    "# Prompt user for the model type\n",
    "f_model = 'tao' # (linear, 'tao', 'DQlearning', 'surr_opt'): \" tao => adaptive_contrast_tao) # Note for linear linear run separate R code\n",
    "\n",
    "contrast = 1\n",
    "\n",
    "surrogate_num = 1 #1- old multiplicative one  2- new one\n",
    "\n",
    "option_sur = 1 # if surrogate_num = 1 then from 1-5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "\n",
    "\n",
    "# Constants from scheme_i\n",
    "C1 = C2 = 3\n",
    "beta = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# network_parameters_surogate = {\n",
    "#   'setting': setting,\n",
    "#   'n_epoch': 220,\n",
    "#   'num_networks': 2,\n",
    "#   'input_dim_stage1': 2,\n",
    "#   'output_dim_stage1': 1,\n",
    "#   'input_dim_stage2': 5,\n",
    "#   'output_dim_stage2': 1,\n",
    "#   'optimizer_betas': (0.9, 0.999),\n",
    "#   'optimizer_eps': 1e-08,\n",
    "#   'scheduler_step_size': 30,\n",
    "#   'scheduler_gamma': 0.8,\n",
    "#   'hidden_dim_stage1': 10, #20\n",
    "#   'hidden_dim_stage2': 10, #20\n",
    "#   'dropout_rate': 0.0, #0.3, 0.43\n",
    "#   'optimizer_lr': 0.1, # 0.07\n",
    "#   'optimizer_weight_decay': 0.001,\n",
    "#   'batch_size': math.ceil(batch_prop*sample_size), #int(0.038*sample_size),\n",
    "#   'f_model': 'surr_opt',\n",
    "#   'option_sur': option_sur, # if surrogate_num = 1 then 5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "#   'contrast': contrast\n",
    "\n",
    "# }\n",
    "\n",
    "\n",
    "# #BEST so far for tao's 15000\n",
    "\n",
    "network_parameters_surogate = {\n",
    "  'setting': setting,\n",
    "  'n_epoch': 60, #250\n",
    "  'num_networks': 2,\n",
    "  'input_dim_stage1': 2,\n",
    "  'output_dim_stage1': 1,\n",
    "  'input_dim_stage2': 5,\n",
    "  'output_dim_stage2': 1,\n",
    "  'optimizer_betas': (0.9, 0.999),\n",
    "  'optimizer_eps': 1e-08,\n",
    "  'scheduler_step_size': 30,\n",
    "  'scheduler_gamma': 0.8,\n",
    "  'hidden_dim_stage1': 10, #20\n",
    "  'hidden_dim_stage2': 10, #20\n",
    "  'dropout_rate': 0.0, #0.3, 0.43\n",
    "  'optimizer_lr': 0.07, # 0.07, 0.007\n",
    "  'optimizer_weight_decay': 0.001,\n",
    "  'batch_size': math.ceil(batch_prop*sample_size), #int(0.038*sample_size),\n",
    "  'f_model': 'surr_opt',\n",
    "  'option_sur': option_sur, # if surrogate_num = 1 then 5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "  'contrast': contrast\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# # to ovefitting model\n",
    "# network_parameters_surogate = {\n",
    "#   'setting': setting,\n",
    "#   'n_epoch': 250,\n",
    "#   'num_networks': 2,\n",
    "#   'input_dim_stage1': 2,\n",
    "#   'output_dim_stage1': 1,\n",
    "#   'input_dim_stage2': 5,\n",
    "#   'output_dim_stage2': 1,\n",
    "#   'optimizer_betas': (0.9, 0.999),\n",
    "#   'optimizer_eps': 1e-08,\n",
    "#   'scheduler_step_size': 30,\n",
    "#   'scheduler_gamma': 0.8,\n",
    "#   'hidden_dim_stage1': 5,\n",
    "#   'hidden_dim_stage2': 5,\n",
    "#   'dropout_rate': 0.00, #0.3\n",
    "#   'optimizer_lr': 0.01,\n",
    "#   'optimizer_weight_decay': 0.001,\n",
    "#   'batch_size': math.ceil(batch_prop*sample_size), #int(0.038*sample_size),\n",
    "#   'f_model': 'surr_opt',\n",
    "#   'option_sur': 1 # if surrogate_num = 1 then 5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# network_parameters_surogate = {\n",
    "#   'setting': setting,\n",
    "#   'n_epoch': 250,\n",
    "#   'num_networks': 2,\n",
    "#   'input_dim_stage1': 2,\n",
    "#   'output_dim_stage1': 1,\n",
    "#   'input_dim_stage2': 5,\n",
    "#   'output_dim_stage2': 1,\n",
    "#   'optimizer_betas': (0.9, 0.999),\n",
    "#   'optimizer_eps': 1e-08,\n",
    "#   'scheduler_step_size': 30,\n",
    "#   'scheduler_gamma': 0.8,\n",
    "#   'hidden_dim_stage1': 20,\n",
    "#   'hidden_dim_stage2': 20,\n",
    "#   'dropout_rate': 0.43, #0.3, 0.43\n",
    "#   'optimizer_lr': 0.01,\n",
    "#   'optimizer_weight_decay': 0.001,\n",
    "#   'batch_size': math.ceil(batch_prop*sample_size), #int(0.038*sample_size),\n",
    "#   'f_model': 'surr_opt',\n",
    "#   'option_sur': option_sur, # if surrogate_num = 1 then 5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "#   'contrast': contrast\n",
    "\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# # #BEST so far for linear's 15000\n",
    "\n",
    "# network_parameters_surogate = {\n",
    "#   'setting': setting,\n",
    "#   'n_epoch': 120, #60, 120\n",
    "#   'num_networks': 2,\n",
    "#   'input_dim_stage1': 2,\n",
    "#   'output_dim_stage1': 1,\n",
    "#   'input_dim_stage2': 5,\n",
    "#   'output_dim_stage2': 1,\n",
    "#   'optimizer_betas': (0.9, 0.999),\n",
    "#   'optimizer_eps': 1e-08,\n",
    "#   'scheduler_step_size': 30,\n",
    "#   'scheduler_gamma': 0.8,\n",
    "#   'hidden_dim_stage1': 40, #3,\n",
    "#   'hidden_dim_stage2': 40, #5,80 increasing to 80 made overall good value func. stablity\n",
    "#   'dropout_rate': 0.5, # 0.43, 0.5\n",
    "#   'optimizer_lr': 0.1, # 0.006\n",
    "#   'optimizer_weight_decay': 0.03, #0.003\n",
    "#   'batch_size': math.ceil(batch_prop*sample_size), #int(0.038*sample_size),\n",
    "#   'f_model': 'surr_opt',\n",
    "#   'option_sur': 1, # if surrogate_num = 1 then from 1-5 options, if surrogate_num = 2 then 1-> assymetric, 2 -> symmetric\n",
    "#   'contrast': contrast\n",
    "# }\n",
    "\n",
    "network_parameters_qlearning = {\n",
    "  'setting': setting,\n",
    "  'n_epoch': 240,\n",
    "  'num_networks': 1,\n",
    "  'input_dim_stage1': 6,\n",
    "  'output_dim_stage1': 1,\n",
    "  'input_dim_stage2': 8,\n",
    "  'output_dim_stage2': 1,\n",
    "  'optimizer_betas': (0.9, 0.999),\n",
    "  'optimizer_eps': 1e-08,\n",
    "  'scheduler_step_size': 30,\n",
    "  'scheduler_gamma': 0.8,\n",
    "  'hidden_dim_stage1': 5,\n",
    "  'hidden_dim_stage2': 5,\n",
    "  'dropout_rate': 0,\n",
    "  'optimizer_lr': 0.07,\n",
    "  'optimizer_weight_decay': 0.03,\n",
    "  'batch_size': math.ceil(batch_prop*sample_size),\n",
    "  'f_model': \"DQlearning\"\n",
    "}\n",
    "\n",
    "# network_parameters_qlearning = {\n",
    "#   'setting': setting,\n",
    "#   'n_epoch': 120,\n",
    "#   'num_networks': 1,\n",
    "#   'input_dim_stage1': 3,\n",
    "#   'output_dim_stage1': 1,\n",
    "#   'input_dim_stage2': 6,\n",
    "#   'output_dim_stage2': 1,\n",
    "#   'optimizer_betas': (0.9, 0.999),\n",
    "#   'optimizer_eps': 1e-08,\n",
    "#   'scheduler_step_size': 30,\n",
    "#   'scheduler_gamma': 0.8,\n",
    "#   'hidden_dim_stage1': 40,\n",
    "#   'hidden_dim_stage2': 40,\n",
    "#   'dropout_rate': 0.43,\n",
    "#   'optimizer_lr': 0.01,\n",
    "#   'optimizer_weight_decay': 0.03,\n",
    "#   'batch_size': math.ceil(batch_prop*sample_size),\n",
    "#   'f_model': \"DQlearning\"\n",
    "# }\n",
    "\n",
    "\n",
    "# standard q learning params\n",
    "\n",
    "network_parameters_qlearning = {\n",
    "  'setting': setting,\n",
    "  'n_epoch': 240,\n",
    "  'num_networks': 1,\n",
    "  'input_dim_stage1': 3,\n",
    "  'output_dim_stage1': 1,\n",
    "  'input_dim_stage2': 6,\n",
    "  'output_dim_stage2': 1,\n",
    "  'optimizer_betas': (0.9, 0.999),\n",
    "  'optimizer_eps': 1e-08,\n",
    "  'scheduler_step_size': 30,\n",
    "  'scheduler_gamma': 0.8,\n",
    "  'hidden_dim_stage1': 5,\n",
    "  'hidden_dim_stage2': 5,\n",
    "  'dropout_rate': 0,\n",
    "  'optimizer_lr': 0.07,\n",
    "  'optimizer_weight_decay': 0.03,\n",
    "  'batch_size': math.ceil(batch_prop*sample_size),\n",
    "  'f_model': \"DQlearning\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# input_stage2 = [O1, A1, Y1, O2]\n",
    "\n",
    "if setting =='tao':\n",
    "    network_parameters_surogate['input_dim_stage1'] = 5\n",
    "    network_parameters_surogate['input_dim_stage2'] = 7\n",
    "    network_parameters_qlearning['input_dim_stage1'] = 6\n",
    "    network_parameters_qlearning['input_dim_stage2'] = 8\n",
    "\n",
    "\n",
    "if setting=='scheme_i':\n",
    "    network_parameters_surogate['input_dim_stage1'] = 3\n",
    "    network_parameters_surogate['input_dim_stage2'] = 6\n",
    "    network_parameters_qlearning['input_dim_stage1'] = 4\n",
    "    network_parameters_qlearning['input_dim_stage2'] = 7\n",
    "\n",
    "    # Define g as a lambda function\n",
    "    g = lambda O1: 1.5 * (O1[:, 2] > 0)\n",
    "\n",
    "    # Define f_i lambda functions for each scheme\n",
    "    if scheme == 1:\n",
    "        f_i = lambda O1, A1, i: 1.5 * A1 * O1[:, i-1] + A1 * (O1[:, i-1] ** 2) / 2\n",
    "    elif scheme == 2:\n",
    "        f_i =  lambda O1, A1, i: 1.5 * (O1[:, i-1] > 0).float() * A1 + A1 * (O1[:, i-1] ** 2) / 8\n",
    "    elif scheme == 3:\n",
    "        f_i = lambda O1, A1, i: 1.5 * (np.sin(O1[:, i-1]) > 0) * A1 + A1 * (O1[:, i-1] ** 2) / 8\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scheme number. Please choose 1, 2, or 3.\")\n",
    "\n",
    "\n",
    "network_parameters_surogate['option_sur'] = 2 # symmetric\n",
    "n_epoch = network_parameters_qlearning['n_epoch']\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Lists to store replication results\n",
    "V_replications_M1_behavioral = []\n",
    "V_replications_M1_pred = []\n",
    "V_replications_M1_optimal = []\n",
    "\n",
    "V_replications = {\"V_replications_M1_behavioral\": V_replications_M1_behavioral, \"V_replications_M1_pred\": V_replications_M1_pred, \"V_replications_M1_optimal\": V_replications_M1_optimal}\n",
    "\n",
    "\n",
    "print('Setting: ' , setting)\n",
    "print(\"f_model: \", f_model)\n",
    "\n",
    "if f_model == \"DQlearning\":\n",
    "    V_replications, df, values_opt, losses_dict, epoch_num_model_lst = simulations(surrogate_num,\n",
    "                                                                                   sample_size,\n",
    "                                                                                   num_replications,\n",
    "                                                                                   V_replications,\n",
    "                                                                                   network_parameters_qlearning)\n",
    "elif f_model == \"surr_opt\":\n",
    "    V_replications, df, values_opt, losses_dict, epoch_num_model_lst = simulations(surrogate_num,\n",
    "                                                                               sample_size,\n",
    "                                                                               num_replications,\n",
    "                                                                               V_replications,\n",
    "                                                                               network_parameters_surogate)\n",
    "\n",
    "else:\n",
    "    network_parameters_surogate['f_model'] = 'tao'\n",
    "    V_replications, df, values_opt, losses_dict, epoch_num_model_lst = simulations(surrogate_num,\n",
    "                                                                               sample_size,\n",
    "                                                                               num_replications,\n",
    "                                                                               V_replications,\n",
    "                                                                               network_parameters_surogate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24959719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa1445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf972f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_replications[\"V_replications_M1_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average V and standard deviation for test data\n",
    "V_replications_M1_pred = V_replications[\"V_replications_M1_pred\"]\n",
    "\n",
    "avg_V_test_M1 = np.mean(V_replications_M1_pred)\n",
    "std_dev_test_M1 = np.std(V_replications_M1_pred)\n",
    "print(f\"Average V for Test Data, M1: {np.mean(avg_V_test_M1)}\")\n",
    "print(f\"Standard Deviation for Test Data, M1: {std_dev_test_M1}\\n\\n\")\n",
    "\n",
    "plt.plot(range(1, num_replications + 1), V_replications_M1_pred, 'o-')\n",
    "plt.xlabel('Replications_M1: 500')\n",
    "plt.ylabel('V Value')\n",
    "plt.title(f'V values for {num_replications} Test Replications (Test Sample Size: {sample_size})')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running totals for overall accuracy\n",
    "correct_behavioral_A1 = 0\n",
    "correct_behavioral_A2 = 0\n",
    "correct_predicted_A1 = 0\n",
    "correct_predicted_A2 = 0\n",
    "total_A1 = 0\n",
    "total_A2 = 0\n",
    "\n",
    "# Initialize lists to hold per-simulation accuracies\n",
    "accuracies = {\n",
    "    'Accuracy_A1': [],\n",
    "    'Accuracy_A2': []\n",
    "}\n",
    "\n",
    "# Iterate over each row in the DataFrame for row-wise accuracy\n",
    "for index, row in df.iterrows():\n",
    "    behavioral_A1 = row['Behavioral_A1']\n",
    "    behavioral_A2 = row['Behavioral_A2']\n",
    "    predicted_A1 = row['Predicted_A1']\n",
    "    predicted_A2 = row['Predicted_A2']\n",
    "    optimal_A1 = row['Optimal_A1']\n",
    "    optimal_A2 = row['Optimal_A2']\n",
    "\n",
    "    # Calculate row-wise accuracy for A1 and A2\n",
    "    row_correct_behavioral_A1 = sum(a == p\n",
    "                                    for a, p in zip(behavioral_A1, optimal_A1))\n",
    "    row_correct_behavioral_A2 = sum(a == p\n",
    "                                    for a, p in zip(behavioral_A2, optimal_A2))\n",
    "    row_correct_predicted_A1 = sum(o == p\n",
    "                                   for o, p in zip(optimal_A1, predicted_A1))\n",
    "    row_correct_predicted_A2 = sum(o == p\n",
    "                                   for o, p in zip(optimal_A2, predicted_A2))\n",
    "\n",
    "    # Store per-simulation accuracies\n",
    "    accuracies['Accuracy_A1'].append(row_correct_predicted_A1/len(predicted_A1))\n",
    "    accuracies['Accuracy_A2'].append(row_correct_predicted_A2/len(predicted_A2))\n",
    "\n",
    "\n",
    "    # Update running totals for overall accuracy\n",
    "    correct_behavioral_A1 += row_correct_behavioral_A1\n",
    "    correct_behavioral_A2 += row_correct_behavioral_A2\n",
    "    correct_predicted_A1 += row_correct_predicted_A1\n",
    "    correct_predicted_A2 += row_correct_predicted_A2\n",
    "    total_A1 += len(predicted_A1)\n",
    "    total_A2 += len(predicted_A2)\n",
    "\n",
    "# Convert accuracies dictionary to DataFrame\n",
    "accuracy_df = pd.DataFrame(accuracies)\n",
    "\n",
    "# Calculate overall accuracies\n",
    "overall_accuracy_behavioral_A1 = correct_behavioral_A1 / total_A1\n",
    "overall_accuracy_behavioral_A2 = correct_behavioral_A2 / total_A2\n",
    "overall_accuracy_predicted_A1 = correct_predicted_A1 / total_A1\n",
    "overall_accuracy_predicted_A2 = correct_predicted_A2 / total_A2\n",
    "\n",
    "# Print the overall accuracies\n",
    "print(\"Overall Accuracy for Behavioral A1:\", overall_accuracy_behavioral_A1)\n",
    "print(\"Overall Accuracy for Behavioral A2:\", overall_accuracy_behavioral_A2)\n",
    "print(\"\\n\")\n",
    "print(\"Overall Accuracy for predicted A1:\", overall_accuracy_predicted_A1)\n",
    "print(\"Overall Accuracy for predicted A2:\", overall_accuracy_predicted_A2)\n",
    "\n",
    "accuracy_df[\"Value function\"] = V_replications[\"V_replications_M1_pred\"]\n",
    "accuracy_df[\"Optimal Value function\"] = V_replications[\"V_replications_M1_optimal\"]\n",
    "\n",
    "# Print the DataFrame with per-simulation accuracies\n",
    "print(\"\\n \", accuracy_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824b505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc59ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578f489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
